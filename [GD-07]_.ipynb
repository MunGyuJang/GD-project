{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96f0b1df",
   "metadata": {},
   "source": [
    "# 프로젝트 : BERT pretrained model 제작\n",
    "\n",
    "**노드를 보면서 문득 생각이 들었습니다.  \n",
    "코드를 이해는 했는데 마냥 복사+붙여넣기를 하기엔 너무 재미없을 것 같아서 코드골프를 하는 컨셉을 잡으면 어떨까!?!?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2943ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8092bc",
   "metadata": {},
   "source": [
    "## 1. Tokenizer\n",
    "\n",
    "## 2. MLM\n",
    "\n",
    "## 3. NSP\n",
    "\n",
    "## 4. 데이터셋 생성\n",
    "\n",
    "## 5. BERT 설계하기\n",
    "\n",
    "## 6. BERT pre-train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523aec4f",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c986bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_8000'\n",
    "vocab_size = 8000\n",
    "\n",
    "#spm.SentencePieceTrainer.train(\n",
    "#    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "#    \" --model_type=bpe\" +\n",
    "#    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "#    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "#    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "#    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "#    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "#    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc784b47",
   "metadata": {},
   "source": [
    "SentencePiece `ko8000` 모델의 학습은 처음 실행 때만 진행하고 이후는 불러오기만 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b25e9f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf73bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8007"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743124",
   "metadata": {},
   "source": [
    "SentencePiece 모델의 Vocab Size는 스페셜 토큰 7개를 포함한 8007개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8232f2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[BOS]', '[EOS]', '[SEP]', '[CLS]', '[MASK]', '▁1', '▁이', '으로', '에서']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = [vocab.id_to_piece(id) for id in range(len(vocab)) if not vocab.is_unknown(id)]\n",
    "# 단어를 마스킹하면서 무작위로 다른 토큰으로 변경하려고 할 때 꺼내올 토큰들 리스트\n",
    "\n",
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78d673",
   "metadata": {},
   "source": [
    "단어장에서 스페셜 토큰이 아닌 유효한 토큰들만을 담은 리스트를 준비해줍니다.\n",
    "\n",
    "앞에서 6개의 단어가 스페셜 토큰으로 이루어져있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b2bf209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_list = vocab_list[6:]\n",
    "\n",
    "vocab_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70826f",
   "metadata": {},
   "source": [
    "슬라이싱으로 제거할 경우 인덱스 6부터 슬라이싱하면 되겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58af838",
   "metadata": {},
   "source": [
    "- - -\n",
    "## MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c5dde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b9e68",
   "metadata": {},
   "source": [
    "다음과 같은 토큰이 있다고 칩시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e4f94",
   "metadata": {},
   "source": [
    "![화면 캡처 2022-12-15 181359.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAxYAAAI7CAYAAABx+w37AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAFiUAABYlAUlSJPAAACyFSURBVHhe7d2NsuMosjDar+77v/O5TdQQzTD6AQMCpLUiiK5tYymVYrczLXv7z//94/8BAAA0+P/+818AAICfaSwAAIBmGgsAAKCZxgIAAGimsQAAAJppLAAAgGYaCwAAoJnGAgAAaKaxAAAAmmksAACAZhoLAACgmcYCAABoprEAAACa/fm/f/zn37f+/Pnzn3/9VfFQDjyVz1XPWx5XamSMYb9fWLuz8rubnuth1d81AHhCdWNROn1m8bbCk3ttDCPztct5+1We6yPpMdUe4wo5EcNfJTGEOXfSbdQeV+n8FfIFAE963Vuh4pN5OkoKjZ5WiOFL8lyf3cY3HJ37o9sAgL66NxahgJ5VRMeCPhdueyqmFWJYWchB7aDcUf7uBgBAD90bi1BAHxXWEMT1kY+7+1p8qZA+yl8Yd/d9yZfWAwA8yV+F6mz3Qm1G/KG4C/vtUeQdbSv8nI5avzymt5YYeuV3xzy8dT0AwIpe1Vis/IQfCxz+FXKS5iX8N95GO/kFAJ70yr8KlZsRS1683e1/ZIyrnIuanNTmL8iP8+y4V8lHb6Pzu5vZ62HUdgFgVa9vLHaJY2Scq+RgpLNjPLr9C/n4uhXWg3UGwNe89jMW4Ul9pSf2EEeIh77uznPMu9x/g/UAAPO8srGIhcVZccF7lJxna+E7rAcAmOd1b4Watd8SV7GNjHuFnIQYWq16Xlcgv+tZ4fcOAJ6ksejsav+/3tdqdk5KtMQYHltj9VyMsMMa6GWV9fClnANA8KrGYoUn8qsYfr2v1Qp5ufNkjDvko7cvHnOpUbmRcwC+ZonPWIQn4Nl6xRAKiaNtKTK+YYW1vAJ5AIDved2Ht0NBczaeEpuLdGgqAAB4s+rGIi2Wr8wopMM+r8aTSvZdmssentwXfJXfMwC+rOozFiOEJ+DJISwRw9c9eQ5G7WvldfSG/I6yW7wAsKrpjQV7CMXXLywvAIBv0FgAAADNXvnN2wAAwLM0FgAAQDONBQAA0ExjAQAANNNYAAAAzTQWAABAs6o/N5t/l8Gvf6m213buPLWf3kZ9YVfP7e6aWwAAxqi+YhEKyDhK5AVoULuNFqX7Ooqzh7vt/rrf8Lir0cPVdkrzCgDANwx9K1SvApf/lRb2R0PuAQB4ks9YAAAAzYY1FuEV8/DKOQAA8H6uWPyHJuh/yQkAAKWGNBauVgAAwLe4YvGA0GidDQAAeIPujUUoll2t+G8hH2fjaZoaAABGcMXiha6au5lNDQAA79W1sXC1AgAAvskVi5fR3AEAMEO3xiK+Zz/8Nx3pbYwVcqypAABghm6NRXzffj7S+xhHUwEAwEzeCjVYKPbjFZuj0doM9NoOAAC00Fg8IBT9Z+MXeWPy63YAAKCXP/8UpcVVaShkUy2FcWpUYfzUfnqLDUNvPbe7a24BABijqrHgGTs0FgAAkNJY3MhfmS8lrQAAfInGAgAAaObD2wAAQDONBQAA0ExjAQAANNNYAAAAzTQWAABAM1+Qt4j8Oybyn3uo2eaI/ZfY6ZwBAPCv6sbianpeFKbOHjeygK3Z9qg4Srebz7t7XLj/ytFja46xZm6t0m2PjAEAgL66vxUqFIJHg76OcpyOu8YDAAB68hkLAACgmcYCAABoprH4j/D2If4V305VM0rJNQDA+2gsOBU/r1E6AAD4ru6NRcsr2V9Xm7N0fj4U+gAAPKl7Y5G/ih1GKHS5l+bsTmwezgYAADypa2NxVtCG2zUXe4hXPFoGAADf4zMW/Jf0qsevAwCA79FYAAAAzTQWAABAs66NhffX1xn9mYS4/V/2MTIuAADep/uHt48K0nDbV997H3NyNFo+k3C13Tji9n/dBwAAlPrzT9FZXHXGYvVOmJe6ekzpNn8xctu95bGOiL1mmyvkbqfzBwDwddVXLEKxF8eZUAymI1eyjV6e3BftnC8AgD1VXbHgOaGw7n1qardZW9xbSgAA36WxuPHrK+fSCgDAl2gsAACAZr7HAgAAaKaxAAAAmmksAACAZhoLAACgmcYCAABoprEAAACaVf252fw7Hfyl2jZP5XPX8xbitsbG5UF+/+qZh11/1wCgh+rG4m76zCfWfN+5mbGU5G1UfDXbHhXHL9utfczIHJaqzXV09ZhftxmdPbY2X7XzR/g1F1ePGZWHFfIFAE/q+lao+ESajvTJfbR83+l40uw8sL58jfRYH0frrte2dzMivwDAtW6NRXwiz33tSV0euHO0RlrXx9m6C7629kbkFwC498iHt88KnqdcFV3wFeH3IB0AAD35q1D8l9UasJpCeIXY72IIx3E2J9w+suAP209HrV8e09tdDE/kd4U8AMCKNBadnRUvVwUP50LO4gAAYF3dG4tQQKdjthkFfdhfngeF8b00X9SJa+6I9fff0nV2ljMAoF7XxiIWMOn44hO3PPyvmIOrkeaLemc5ls//lq4zuQGAfro2FkdP0uG2UNx8xVkh97U8HAk5uBq0k1cAYJZXf8bCq7V7c+4AAPbhw9s8JjR6NYOxNG4AQE8aCx4T35pTOt7YXFwdV7g93N/DVe7e3LQ9lV8A4H9pLAAAgGbdGouzVwpLXiVc4RXUXjG05OELQh5Kx8h8he3PcrRGRh/v0+QXAL6n6xWL+ISeji8+mcvDsZiH0vFm4fhGrY+4vfDf3NV9bzIyvwDAsT//POEWP+PmxUivJ+sVnvhnxDAqn7mn9nNnhfMcrRTLnZpY87npz1f39TZy2731jDVsK7VLDgCgh6rGYoQVCpCdiqDd5YXXnRHnZbfzXRJvzOvRvF/v+9Ub8wsA3JveWLCHWIDWsrwAAL5BYwEAADTz52YBAIBmGgsAAKCZxgIAAGimsQAAAJppLAAAgGZLfEHe6p467q/m98xXvl/g6k/59j5+awwAGKW6sbibvlLhUlqYpjGXHN+oY6rZ9ug4ao3IW+1jRuakVE0MMc9X80vm5EpjWCFfAMB7dH0rVCxU0hELo1XlMa8e7xPSfJQOeauTrrsr8gsA7KJbYxELpdyMoijsr2SfRzEr4gAAoN4rP7wdmoOjJgcAABjjs38V6uhqRbTDVYuvNE7hPMRxZ4WclMYQ19jdccU5Ncf2lbUBAKzFn5tlaaFIjuNt4nHF5uFovPXYAYD30ViwjLSg/pLYPBwNAIBddGssQhF0VBB+rUjkWFwfV0NBDQCwr65XLI6KR0UiUdo4HA0AAPbV/QvyjsxqMK72exfT2f0jj2VWnnrYOfYnhTy16pVn5wwA6EljceLs/pHHMitPPZTEHubU2DUXu9h5vQEA6+n2VqjaonG2UFCdxazgqlOarzCnZuy2pgAAvsxfhYIJNE0AwNs88lehwn1XZhVZRzGXxMt/q8lXyG/pGHkewvYBAOin+2cs8oKtZPOjisjS7aYxlxzfiFiDkdsulebiV2fHsMLxRbNjWeVcr3I+AID9VTcWqV5FyeoFzqjjzj21n1lWOs+zY5m1/7evMQBgnqrGYoSVik3GywvbOyPWxiprboVcAAD0Mr2xoK/aYjWyDAAAaKGxAAAAmvlzswAAQDONBQAA0ExjAQAANNNYAAAAzTQWAABAM40FAADQbIlv3maep86ptTNOyG2az/znL+qZA2sXAMpUNxal00fNHaUkhpUKjBHxjjwPNdseGUf0xD7ulMYQ5qXyx+TbqTm2HfKQH/+VuJ3a4yqdv0K+AGBV3gpVKBYU6agpeJ62W7wccx7/Ngv5uLodAJije2MRip7Swqdm7iglMYT7j4qWcNvT8e8WL79zHgGAnXRvLELRc1QMHamZO8oKMdTYLV4AAL7BW6FeaPfG44n4V8jRXQzh/qMrE0dXMsJtcdTYIQ9PWCEGANidxqLAW4qOo4KUtYXzlTYNZ+cw3BYHAMAMGosGqxfqJQUp60sbBufw39+78F8AYB0aix/tUKjHgjQOhRgAAKNoLCqF4nyHpuKI5oLdpb971jMArEVjUSEWNTs2FewlrLV8HN0OALAKjUWhUMRpKHhKbGDvxpcc/Q6GnzVYALAGjcULKbQAAHiaxqKAqxXMUtIkfmFtXv0OumoBAGvYrrHYrYCYEe9ZoXVVnHFutzU36hzPykPJutVcAMB8rlgUCkXL2VhRLLTSoalgNzXr1voGgLn+/PNkXPxsHJ7kUzOeyHcrkFeP96lzusLa+dXMc7jS+ll9Lad6xrrz2gWAJ1U1FrPtVNgEu8XL/1rhHOaFbYneMfvdAwDubNVY8G2/FNiBJQ4AMJ7GAgAAaObD2wAAQDONBQAA0ExjAQAANNNYAAAAzTQWAABAs+2+IG91T+Voh3PhuwTOjcpNz+3usMYAgHVUX7EIxUUcV/KipMQvj+nlbt/h/rORKs1PD6X7ymPcyQqx38XwRIxhH3H0dLW90vUFABAs81aoJ4qzVmmhlQ4YKfxupOtth98VAOB7ujcWoejZqfDZLd5dhJwqgtvFPKbkFQBYUffGIr6qWuOoeHrKL/FyLT2f4b+KYACA9/NXoT7gqcYpNBBHTWJsLn5pMJ6K/cqsGI5yGcWctlohvwDAO0xvLK6KJ9YXG4Z4Hq8K4TDS+QAAvIcrFhXSolhx/FdsGI4aiqP8XM1/M2sGAHi7qY1FKLJ2KjDTojgOhSJxHVyNdM0AALyRKxaFzgrCWFTybWnjcDQAAN5uWmMRX8VlX+kr8kejdA4AAPtzxYKf5a/K/zK+6svHDgC805TGIr5SffTqdfpv2EW6jkvHndB8nM0Lt2tOAICVTGks4qvV+UjvW01JIfhVcvNXupbvBgDA22z3VqhZRWwoBo/27ZXjdbyxwTlad9YcALAin7GoEIu8dCjwGC1fd9YcALCiP/8UKcVVSihqUjMKnNULq6dytMK5iFY5JzPjqN33qFh7bjdsK7XCOQYA1lXVWMw2s3DkWl6E3ul9HldYGzU5GBWr3xEAYJatGgv6qm0GIksGAICcxgIAAGjmw9sAAEAzjQUAANBMYwEAADTTWAAAAM00FgAAQDONBQAA0Gz5b95+ap8zjq2HUV+I1nO7u+YWAIBy1Y1F6fSSubUFZ89iN9f72H6R5yN3tM/aWNJ9XD3ul+2WzK/dLgAAe5j2VqhYYKYjLXq/Ks9JHD3kOZdvAAB66d5YhGL1rmCNBW5OsTvOUc7lGwCAXro3FqFYPWoaSrU8FgAAmMNfhfrBas1PuOoQx5GjqxVRuL3HVQsNIQDAt2ksXiAU9XEAAMAMUxuL9JX2Hq+aAwAAc0xrLEIjkb7SHobm4n+brThmmL1/AAD2Ma2xOHrbztebi7TJOhpHzm7v4W7fAAAQ+YzFZtKrCHEAAMBsGovF3DUK6VUEVxMAAFiFxmIT+VWKo3EmNB9n94fbNScAALTSWGwivUJxNgAAYJYpjUUogo9eQffq+b2Qo3yUOMp5+Fm+AQDoYdoVi1jopkORey3mKB/h9hJxrnwDANDbn3+Ky+LqMi9gnyhMn9rnjGM7clXwn9139Zhf9dzmKrkFAGCcqsaCZ+SFeBRO1dF9I07hiGYFAID30lh0dNYQ3HEKAADYncYCAABo5s/NAgAAzTQWAABAM40FAADQTGMBAAA001gAAADNfEHef8w4titHf7r2KKYwr1esq+UAAIB9VDcWpdOv5h4Vzamrx1WEW6XXsT2tNpbS+SsdIwAA65vyVqhQsJ4NAABgP90bi/BKdxj8JubvagAAwGq6NxYtVx5C0fz1qxYxf2kej24DAICV+KtQP5hZ3LdevdCYAAAwgsZiYfEKTtpAxKsWcQAAwAqWaSxiEc1faT7Cf3+5OgEAAE9xxWJBR02W5gIAgJVpLBYSGoejpiKKzYUGAwCA1SzRWFwV018ScnCXh5I5AADwNFcsNnJ0pUKTAQDACjQWAABAM43FJuLbxXy+AgCAFWksNpB+BkVzAQDAijQWi0ubikhzAQDAav78U6QWf/o3L2YrHvqzp/Y549iuxHiu4iiZU2O1HAAAsI+qxoLxfmkWejcYAABQS2PRUf6KfymnAACA3WksAACAZj68DQAANNNYAAAAzTQWAABAM40FAADQTGMBAAA001gAAADNfPP2f8w4th5C3L1izbfVa9u75hYAgHLVVyxCURjHlbyYPBLmpONI6f56KN1XybH94m67PfZ7lesWV9sszSsAAPua9laoUIimBWcYIwpe/pXmXK4BAOipe2MRCta7ojUWuDkF7zh5zuUaAICeujcWoWA9ahooE4r9s3F0PwAArMBfhfrByMYpNmZH4+j+EqEBOZobbuvVnJTGAgDAO01pLM4K2rMCGAAAWNu0KxaxuUiHpmKOmH8AAPjVtMYiNhLpUNzOEfMPAAC/mtJYnF2d+HpzEY//bCj+AQBYlQ9vLyY0D2cDAABWpbEAAACaaSw+IFztCG+lynl7FQAAvWgsFnPUAFzRGAAAsIIpjYVX0J+X51yuAQDoadoVi1jopkOhO1aac7kGAKCnP/8UmMUVZihIU08Up0/tc8axHZlZ9Of77hXLKrkFAGCcqsaCZ+SF+J1Rp3BmkwMAwF40Fh3VNgSRUwAAwO40FgAAQDN/bhYAAGimsQAAAJppLAAAgGYaCwAAoJnGAgAAaLb8F+Tt5qkc7XoufDfGXz3z4PcSAFhB9RWLULTEcSUvdo6EOemYoTaGu7ml+emhdF8lx9XDiP08FfuV2hiezkPpOgAAGGnaW6FCoZQWRGE8XUTWxpDPfzpeAABYVffGIhTbdwV3LNBzTxbrtTEczX8yXgAAWFn3xiIU20cFO+8WG6+vN1ryAAB8lb8KVSAWi0d2KCKfbPRK8hHuj+POk7Gf+SWG3utihTwAAFz5bGOhUOsnFNBpPu+K6nB/HG9SmwcAgDdxxSKTF4dcO8vX14pqeQAAvm5KY3FWbM0uwM6KQ47d5esrRbU8AABMvGIRi610XBVnI83e/45K8/X2nMoDAMBfU98KFYqtdMwQC8NZ+9+VfP31ax7kDwB4m09/xsJViue9Nd9hLdWonQ8AsLopjcVuRVUohs9i/npzEo6/dgAA8D6fvWIRClxXK/oIeSwdb5Y3UFcDAOBtpjQWocA8Kq7CbXfF56yi7Cjmknh5zqy1EYW1UDoAAN5m2hWLUFyFQjAdTxdc+f7TcSSPWYEIAAB//fmnOC6ujvOCe0ZhvXpB/1SOVjgXQe35GHn+Zq6NmXkI20rNygEA8G1VjcVsMwtHzuWF7ZVR52+FtbFCHgAAZtmqsaCvmkI4ZckAAJDTWAAAAM0+/QV5AABAHxoLAACgmcYCAABoprEAAACaaSwAAIBmGgsAAKDZ8t+8/dQ+ZxxbDyHuXrHumgMAAOarbizupv9SnJYWxz2L6FzNtkfHcSfdd20spfNHHiMAAO/T9a1QsRhNR0mhzL/y/J3dBgAAK+nWWJy9wh1uO2suwu0aDwAA2N/UD297BR4AAN7BX4X6waxmKF7hSUctjRwAACN0aywUrP2FxiHkNTYQ4d/5AACAFQy/YhGLYwAA4L2GNhaait+luQv/jVctAABgRUMai1AEayp+d5Q7zQUAACvr3ljEolhTUe+uIYvNhQYDAIDVdG0sropi7pU0ZCVzAADgacM/vA0AALxft8bC1Yo+at/mJOcAAKzAFQsAAKBZ989YnA0AAOC9/vxfxXtp8gbhibfhPLXPGcd2JMQxc9+pWXEAALCfqsaCZ+QF/h2nEACA2TQWHdU2BJFTAADA7jQWAABAM38VCgAAaKaxAAAAmmksAACAZhoLAACgmcYCAABotvwX5O3mqRytdi5mfPdG2Ge6nfznL+qZg9XWGACwturG4m56TTGyQuEyMt6SfP2qZtsj4wh6bz/P85Gwv3y/K+WkxF0Mv9xfe1yl82u3CwB8T9e3QsXiIx3htiM1c0fZLd6viPlN/53/DADAWro1FrHwzoXb8gK8Zu4ou8ULAAAr8+FtWFhoXM/G0f0AALNoLAocXa3Yyej445WbmjHbCue0JIYw52wc3V/rl8cAABzp1ljUFCgrFDM9YggFssLsr5CHmgEAwLsMv2JRU3yvUKhfxRDuS4cCeb70fAAAMM/QxqKm+F6hUL+LIdyXjq8Xs+H4W0er9HwAADBP9++xCGLB2HvuKC0xXOWkNF+/GLntXlpijI9Nt5Fv7+7nNwjHdOboWEfl4I25BQD66n7FIhYgJUVIzdxRVogBzsS1eTQAAFbStbGoeVVzhVdAV4jhDUIej8bZfTXC+YmPi/8GAGA9wz+8/QaK2Wuh4K8Zd/mMTUSU/hsAgDV1ayzyYvBKzdxRVoiBMum5Cv8NP39F7bFa0wDALNtdsZhRVJ4Vs2nB+3UhF6XjKmd3988QYgIA4Fq3vwp1V3ylj6uZm+tVeP4SQ/6Yuzh6xXpk5LZrzTrO/L5RcczM9cx9p1aJAwBYV3VjkZpRaKxe4DyVoxXORTTrnOT7HRXHrOMLZu87NSsOAGAPVY3FbDOLLK7lReidEedxxPpYYc2tkFsAgDtbNRb0VVuwRpYMAAA5jQUAANDM91gAAADNNBYAAEAzjQUAANBMYwEAADTTWAAAAM00FgAAQLPtvnl7dU/lyLnYWzh/vc6ZtQAArKC6sbib3lLk9Cy2SpXGm89LXT1m1PHUbPvpvB7l6tf9Px37kdoYSuaP2GawQr4AgG/q+laoWNSkI9xWonReT7Xx5nPj4K+Qu6OchhHvAwDgnbo1FrGgzMWicjW7xbu6mM+jnAbxPrkFAHinJT68fVbks4ea8/f25iLmQgMFAHyNvwr1ATs3bSvE/ksMvZuLnc8hAPAN3RqLXwufmle7e1Ko0Vu+lsO/XbkAAL5i+BWLWY3Dr67iDfflg7oCOszbaT2UOjuumtwAAOxsaGNxV0SuVmTexRPuy4ei8a+Yi7N8xPvCvLe5Oy7rBAD4giGNxW5FZEm8Z/cpGv8VchHzkY9439vEY7vzxmMHAEh1byxKi8jSgmy0Nxe9s6T5fHtufz22N+cEAPimro3FKs1Cqd3iZV1hLdWonQ8AsLrhH94+Eouq8N90pLcBAAD7+PN/FS/Zh4L/bPrVfaV6bKNU7b5+OfaRx/Nkrn61Q4y9hGOtNSI3X8o5ALCWKVcsWvxSwPUQirWjfSvk1jFrbURhHZQOAIC36XrF4krJbkqK9F6F/K/x5o+7iqVXrEdGbrvUXQ5L9DyGmTmp3feoWFdYFwDAN1U3FqkZBczqhdNTOVrhXKxm5tqo3XfPWK0FAGAFVY3FbDMLR9a2wtrIC/wr1jEA8DZbNRb0VVMIpywZAAByGgsAAKDZdn8VCgAAWI/GAgAAaKaxAAAAmmksAACAZhoLAACg2fJfkPfUPmccWw8h7l6x7poDAADmq24s7qbXFKe1hWzPIjpXs+3RcdxJ910bS+n8kccIAMD7dH0rVCxG03FWKNfM/ZI8J2e3AQDASro1FmevcIfb8oahZi4AALC+5T687RV5AADYj78K9YNZzU+4mpOPWho3AABG6NZYKFj7C41D+vaw8O98AADACoZfsYjF8ZH4qnscAADAnoY2FndNRfrKexiai3+luZMbAABWN6SxCEVwWhgfObpPAf3XUe7kBgCAlXVvLGJRfNQ4cC3k7qipiGJzocEAAGA1XRuLq6KYeyUNWckcAAB42vAPbwMAAO/XrbFwtaKP2rc5yTkAACuYcsUiFMNHBbTmBAAA9tT9MxZnIxebi3RoKgAAYE9//inmi6v5vEF4ohF4ap8zju3IzAZrlRwAALCfqsaCZ+QF/h2nEACA2TQWHdU2BJFTAADA7jQWAABAM99jAQAANNNYAAAAzTQWAABAM40FAADQTGMBAAA001gAAADNlv/m7d08lSPn4m8O0uPOf/6injkYld+wndTXzxkAvEV1Y3E3vaRoyOfkniw0aoqc2oKoJF+/qtn2yDiO5HkKavZ/9Pgg30Z+XCvn5MhdDGd5iI4eW3tccR8l26rZdunc2ngBgHV1fStULBLSEW7L5XPS8aTSeIOauV8V8nGUp5ir0nwdPT6ML+b7KA9h9JCeqy/mFgDoq1tjEYuU3KpFS028ux3bDDFHR3kK4n3ytYZ8TTs3AECrpT68nRc77KHmvClgAQDeyV+F+oAdm7XQfOSj53GskJNZMZzlMtwW7uthhfwCAM/q1ljsVkjUxHtWcPUudvkr5jUfR8LcOPhLTgCAGYZfsQjFzVlRmCqdN9pZHOG2WKzFsUK8K4i5KdE7b2FbcfBXa07i+gYAqDG0sdit+L6KN96XDsXXv2I+znIS7wvzWFtc3wAANYY0FrsVkXfxnt0Xbgv38VfIR8xJPuJ9reL2viLNYToAAFbTvbEIRU9tERkfM8Mv8XItzecvuQ3zYwGdj1+2t6t4rGcDAGAlXRuLWPjtYrd4vyQtoNMBAMCahn94G1hLaNBCU53TaAMALbo1Fq5W0NNR4Zv7wvkryUPKmgYAZtnuikVtodVDKNaO9qs5WcuoczFjzY2Wr2lrGQBo9eefYqK4mrgqPu6Kr6vd1BQ1vQqgX+LNH3MXR69Yj4zcdqm7HJY4O4YVji+aGcvofcdzeLSPfN8jYpmZWwCgr+rGIjWjIFi9EHkqRyuci5FWOs8zY1lp371iCdtJrXKeAYA2VY3FbDOLLJ6XF6Aleq+PFdZcbR5Gxev3DwC4slVjQV+/FO6BJQMAQE5jAQAANPM9FgAAQDONBQAA0ExjAQAANNNYAAAAzTQWAABAs+2+IG91T+Vo13Mx6rsQRm33zWblbNe1CwBcq75iEYqAOI6EoiEdV2rmjlIbw93cu/z0VLqvkuP6xd12e+w35rr3MfTe3i9KY6iNdfVjK123AMBeur4VKhQTadEQxlmBUTN3lNoY8vlPx/s18g0AsI9ujUUsAnNHBWHN3FFqYzia/2S8XyPfAAB78eFtmoRC/2zQz1F+zwYAwAzVH94+m15z36/beUpNvMHZ/SOPZcU85Y7uD7elfsnb2XavHrOz2mOrmR/m1uid4zefNwD4mm5XLN5SHCh0xgv5jYO50nNRMgAAzgx/K9TqhXqILx2KJwAAqDe0sTgr1MNt4b7c0W2jhVjSMSOGXcV8nY1w/5PSfQMA8Kxun7FIxcLubm5eAIb5pfsYKY/hLqaz+0ceywp5+kVJ3L/ke9d8lArHV6okv6165frt5w0AvqR7Y9FaKKxQaOQx3MV0dv/IY1khT78oifuXfO+ajxZvOOYvnjcAeKuub4VSJHDH+gAAeKcp32MRGpDZamIIxfDZfM3U3xzUjDvyDQCwn26NhYLvm+J5rxmsoaTJAwAoNeWKRSguj4qakuakVzFUG8PR/JJ4+c1K+c7jAADgf3X95u0rR4/LH1MSSu/isjaGdH7J3J6xpkZuu8boOErz/cZcp8f+q1k5K7VCDABAH9WNRWpGQbB6IfJUjlY4F8Eq52NkHKuvuV/NOq5V1i4A0FdVYzHbWwu83eWF4p0R53DU2nj7mlvh3AEA77BVY0FftUVlZMkAAJDTWAAAAM2m/FUoAADgXTQWAABAM40FAADQTGMBAAA001gAAADNNBYAAECz5b95+6l9zji2HkLcvWLdNQcAAMxXfcUiFJtxHAnFaTpKXM27219PpfsqPa5WI/azSq4BAHiXrm+FCkVrWpyG8VQRDgAAzNOtsYhNRe6quQi3azwAAGB/Uz+8Ha9q8L9io6bxAgBgB90aiy81CE8ea+/mQiMHAMAIw69YxFfeKZfnrHdzAQAAvXVvLEIBnA5NRZ2znGkuAABYWffGIhTA6VAMl7trxOQTAIBVDX8rlGK4zF1TEZXMAQCApw1vLCjza8Og0QAAYAUai8XUXt1xNQgAgBV0aywUuAAA8F2uWCwoNGmlAwAAVtCtsTj7kHa4zecA6oR8lQ4AAFhB1ysWodBNX03XVAAAwDf8+afwL678Q6OQeqJpeGqfM47tSG0z1rN5WyUHAADsp6qx4Bl5gX/F6QMAYAUai45qGoKUUwAAwO40FgAAQDN/bhYAAGimsQAAAJppLAAAgGYaCwAAoJnGAgAAaLb8F+Tt5qkc7XouQtzWzbg89NzurmsMAJijurG4m15TjKxQuIyMtyRfv6rZ9ug47qT7ro1lZOyl7mIoyUEUt1NyXHfbPXp8bb5K59duFwD4nq6NxdH9Z4+pmTvK6HhHHk/NtkfGkeudkydjP9OS67PH9jiuo23Ubrd0fo94AYB36/YZi7PCI9wW7kvVzB1lt3gBAGBlPrwNAAA069ZYHL2iv7KaeHc7ttys+MPVnHzUWiH3u5//K28+NgDgWcOvWIRiMi9eVi5mjuI9UzP3C2I+YgMR/p0PAADeqXtjEYrKdNQUk7Xze6iJt2Yu9JCvuXRYfwDASro3FqHYSUcogErMKpRq4q2Z+zXp+ftqbmIOSo89zIvjSNze2QAAWMnwt0KFAuiq0IqF1SqF0l28qZq5b3Z0/uTmvtEI98UBALC74Y3FlVh4Kaz2FM5fPIdHwu1xDgAA7zatsbgqSNlDSVNYMucN0vUc/ps2U/nPAABv1K2x2K1wqolXUcgvvtBQAQBEU65YhEJd0fUOtU3XG897yXoeddUibDMOAICZujUWZ4VTuK1nMdmrgKqJ96ljo02vtVHjaA30Whdx3V2NMCcOAICZ/vxTkBRXJLGQuRLmpI7m53NyV/soiaFGSbxRzdygd6ypkduusUocwQqxlMYwKtbdtgsAvEd1Y5GaUWisXuA8laMVzkWw0vlYfW2kRsXac7urrDEAYA9VjcVsOxWOX5IXoHdGnMPd1saoeP2OAACzbNVY0FdtQxBZMgAA5DQWAABAs6nfvA0AALyDxgIAAGimsQAAAJppLAAAgGYaCwAAoJnGAgAAaLbdN2/T11PndKe1k3/JXP7zm+Tn5c6X8nB0rD3XQr7Pt+YWgO+obizuprc8WfZ80i5VGm8+L/VkzDX5rT0XI/Nfs+1RcZxtN7/97ucro2KvURrDyFh3ykON2m2ucC4A4Cld3woVnxzTEW4rUTqvp9p487lxPKUm3tpjAwCAFt0ai1jI5lYtaN8c727HxjxhPdSMNzk6vrMBANxb4sPb4Yn7qBCmv6/nOS8Ywzhyd/9bhPVQM97k6PjOBgBwr1tjsduTr3jXNfJYw7bzceTu/jO180cQw181MaSNZM+GcoU8AMBThl+xCE/QV0+ud/c/bbV47uwWL+sI6yasn5rxRuG4Qi7y8dbjBYBRujcWaRESn7BXVhNvPjeMp+X7r4kXcrGITsfZ7WG8zdXvULjd7w0AlOveWKRFyN0T89WT+lNq4s3n3s0foXT/Mbclc78i5iAf4XYola4dAOBfw98KtVtBexbvWfE5+/hq4p0d6wpCDvLxNWENXI3SOV/15bUDAFeGNxZnQnHiiZkVvX1dpoXxr+MtwrGcNUr+HwUAdaY1FrALxeW7xeYiH847ANTp1liEJ+JScW76JJ7fNtoT++hpt3hXFddXzfiCo+OO4wtCExEbifTfAEC5KVcs4hN3PtL7VvOVAuvt0vVWOt587mPzcHTcccQ5AABXujUWsQDJxaKll14FTm28vx7fjHh/jZVn9Vobv4rr4W5NxDmj4p2dBwCgjz//FAzFlWZJYZoXCRWbL95+zTbv1Mb7y/xZ8c6ONTVy26ONin12Tmr3/6Y8hH3WCjG+KQcA0Ft1Y5Ga8US42xPw6vE+dU5XWDu/GnUOV1gb8bxcxVEyp8UKeSjVM9aY12iXHADAmarGYradCpBgt3g5NuI8rrY28iI3NTJOv9MA8B5bNRZ821Xxe8USBwAYT2MBAAA08wV5AABAM40FAADQTGMBAAA001gAAADNNBYAAECz7b4gj76eOqe7rp23fm/Br3+6N/ja732+BnqtiV1/JwDgTHVjcTe95Mkyn5N78gm25sl9hUJgZLxh/qhjqtn2yDhSJfupjeWp2K/0jvmXY9otD2FucDQ/307tdkvm1mwTAFbV9a1Q8ckxHeG2XD4nHU8qjTeomTvKbvHCDtLfFb8jAPC7bo1FfHLOrfpkXRPvCse2W7ywg/x3xe8IAPxuqQ9vnxXEAADA2ro1Frs1BOJd187HukLstTGE+Wev0v/a7O+Qh7Nju8pHrRXyAABPGX7FYrerEEfxnhUaKxzbbvllDWHdpOPotrPbAQCOTPmrUEdKtj1CTby/HltPJTFc5fLX+1qN3Pav8pjCz0dWi5s+7n4XUvk66b0mRmwTAJ7W/YpFeHJMx1mxtorSeOMTf8nckUpiOLt9RryriuczzUma1zj4LmsAAOoMfytUeGLeqaA9ijcWobkVju0shnh7Oo6O4etWOIdPy9fFLwMAILfEX4UKhYqit7+Q03Tw19fXW74u8lE6BwAgtURjATOFQtmr8AAAbbo1FrsVZm+OV5F8LuTGK+4EZw2lNQIAv3HFAv7xxasWGlAAoKdujcVTr/71KoZq4m05tt3ifbPVjv/oHK3g6RzNzEP+u/L13xEAaNH1ikV8kk7Hyk/SNfGucGy7xbuSkuP/cn6+LP1dsQYA4HfVX5CXmvEkvNuT/+rxPnVOV1g7vxh5/mavjVXW5ipxnMnj6xVv2E5q5RwAQImqxmK21QuQ3G7x8r9GncNV1kZe3N7pHfOOvyN+rwHg2FaNBd9WWwRHljgAwHgaCwAAoJk/NwsAADTTWAAAAM00FgAAQDONBQAA0ExjAQAANNNYAAAAzZb/5u2n9jnj2Go9+cVco/bVc7s7nDMAgK+ovmIRirc4SuTF35GrObX7a1G6r5JjWsldvC3HEx57NVpdbaP0fAEAMN7Qt0L1KCxZVzi/aXF/NAAA+Iapn7Ho9ar2F8QiXr4AAFjRsMYiFsJXvKpdJs2l5gIAgBX5q1A/eKoZCg3EUYMWm4tdG4wYe0n8T+UaAIA2QxqLUDAqCH+TFt0hh2d5jPel83cRYz87NgAA9uOKxWKuiu6j5uFqfiptQPIx2lP7AQBgnu6NRSgg74pcnpc2IPk4ExuCs6YgPDadczbu9gMAwP5cseBUbAiumoJ0ztkAAOD9ujYW8dVpfpO/0p+P0jkAAPA0VywWkr/S/8tY3Q4xAgBQr1tjEV8tP3oFPf0375Ke65IBAMA7dWss8lfO40jvo8xOBXh6rkuG5gIA4J28FeoDYkF/NsL9AADQQmPxEelVg3wAAECr6sYifaW7REnhejWndn8tntwX7ZwvAIB1/PmnqPeS9YJqi+XepzHsf8TSGLVdAADm0lh09Osr504BAAC701gAAADNfHgbAABo9P/+3/8P+z88I2zNUf8AAAAASUVORK5CYII=)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c52f9",
   "metadata": {},
   "source": [
    "`▁`로 시작하는 토큰은 앞에 내용이 없거나 띄어쓰기가 있기 때문에 단어의 시작을 알리는 토큰입니다.\n",
    "\n",
    "해당 토큰을 기준으로 이어지는 한 단어의 `[단어의 토큰들]`을 묶어주는 것이 첫 번째 목표입니다.\n",
    "\n",
    "`▁`로 시작하는 토큰인지는 `token.startswith(u\"\\u2581\")`를 통해 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc58b3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    }
   ],
   "source": [
    "startwordtoken = [i for i, token in enumerate(tokens_org) if token.startswith(u\"\\u2581\")] + [len(tokens_org)]\n",
    "cand_idx = [[k for k in range(i, j) if tokens_org[k] != \"[SEP]\"] for i, j in zip(startwordtoken[:-1], startwordtoken[1:])]\n",
    "# startwordtoken : 시작을 알리는 토큰들의 인덱스들 리스트\n",
    "# cand_idx : 공백을 기준으로 한 하나의 단어 토큰들 묶음\n",
    "\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc62229",
   "metadata": {},
   "source": [
    "두 줄의 코드로 원하는 결과는 나온 것 같습니다.\n",
    "\n",
    "그렇다면 예제의 코드와 동일한 결과를 출력할 지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0edc858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx_other = []  # word 단위의 index array\n",
    "\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx_other[-1].append(i)\n",
    "    else:\n",
    "        cand_idx_other.append([i])\n",
    "        \n",
    "    # _로 시작하는 토큰이면 리스트의 새로운 인덱스에 append, 아니면 마지막 인덱스에 토큰 추가\n",
    "\n",
    "# 결과확인\n",
    "print(cand_idx_other == cand_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585385ec",
   "metadata": {},
   "source": [
    "결과는 일치하네요.\n",
    "\n",
    "아직 궁금한 점은 한 가지가 더 남아있습니다.\n",
    "\n",
    "이렇게 코드골프를 하면 실행속도가 더 빨라지지 않을까? 하는 호기심이요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc07b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00017595291137695312\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "cand_idx_other = []  # word 단위의 index array\n",
    "\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx_other[-1].append(i)\n",
    "    else:\n",
    "        cand_idx_other.append([i])\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1164f4",
   "metadata": {},
   "source": [
    "예제 코드의 실행 속도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87a3bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001811981201171875\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "startwordtoken = [i for i, token in enumerate(tokens_org) if token.startswith(u\"\\u2581\")] + [len(tokens_org)]\n",
    "cand_idx = [[k for k in range(i, j) if tokens_org[k] != \"[SEP]\"] for i, j in zip(startwordtoken[:-1], startwordtoken[1:])]\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed990f7f",
   "metadata": {},
   "source": [
    "두 줄로 코드골프를 한 코드의 실행속도입니다.\n",
    "\n",
    "코드는 짧아졌지만 실행 속도는 더 느려졌습니다. 이럴수가..\n",
    "\n",
    "리스트를 두 개 생성하는 게 영향이 큰 듯 해보입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c3f4a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.088165283203125\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    cand_idx_other = []  # word 단위의 index array\n",
    "    \n",
    "    for (i, token) in enumerate(tokens_org):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx_other[-1].append(i)\n",
    "        else:\n",
    "            cand_idx_other.append([i])\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0074f4",
   "metadata": {},
   "source": [
    "만약 반복문으로 엄청 많은 실행을 거치면 결과는 어떨까요?\n",
    "\n",
    "예제의 함수는 약 1.08초가 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1cceb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9586496353149414\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    startwordtoken = [i for i, token in enumerate(tokens_org) if token.startswith(u\"\\u2581\")] + [len(tokens_org)]\n",
    "    cand_idx = [[k for k in range(i, j) if tokens_org[k] != \"[SEP]\"] for i, j in zip(startwordtoken[:-1], startwordtoken[1:])]\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f498ef5",
   "metadata": {},
   "source": [
    "코드골프한 코드는 반복 실행시 약 0.95초로 더 빨라진 결과를 볼 수 있습니다.\n",
    "\n",
    "또 예상치 못한 결과가 나왔군요.\n",
    "\n",
    "모델 내에서 병렬로 처리될 코드는 아닌 것 같지만 그래도 일단 써보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f049510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf67def",
   "metadata": {},
   "source": [
    "다음 스텝을 실험하기 위해 토큰 리스트의 인덱스를 섞어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96e7df26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    # 마스크 카운트를 넘으면 종료\n",
    "    \n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "        \n",
    "    # 토큰했는지 카운트는 토큰 단위로 세지만 마스킹은 공백(_) 단위로 마스킹\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f5492",
   "metadata": {},
   "source": [
    "15% 만큼을 무작위로 선택해 마스킹하는 함수입니다.\n",
    "\n",
    "이번에도 위와 같은 결과가 나오도록 코드골프를 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddebf828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '[MASK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '[MASK]', '[MASK]', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "mask_lms = {}\n",
    "\n",
    "i = 0\n",
    "while len(mask_lms) < mask_cnt: # while 조건문을 이용해 break, continue을 아낌\n",
    "    dice = random.random()\n",
    "    for index in cand_idx[i]:\n",
    "        mask_lms[index] = tokens[index]\n",
    "        if dice < 0.8:\n",
    "            tokens[index] = \"[MASK]\"\n",
    "        elif dice < 0.9:\n",
    "            tokens[index] = random.choice(vocab_list)\n",
    "        # else: 위 두 조건에 해당하지 않을 확률이 10%므로 아무것도 하지 않으면 10% 확률로 원본 그대로 유지\n",
    "    i += 1\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa36d5d",
   "metadata": {},
   "source": [
    "17줄의 코드에서 11줄의 코드로 줄였습니다.\n",
    "\n",
    "정확히 일치하는 지는 알기 힘들지만... 맞을 거라고 믿습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "088bf069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00015306472778320312\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    # 마스크 카운트를 넘으면 종료\n",
    "    \n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "        \n",
    "    # 토큰했는지 카운트는 토큰 단위로 세지만 마스킹은 공백(_) 단위로 마스킹\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b1bfa",
   "metadata": {},
   "source": [
    "예제의 코드 실행속도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c204c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013327598571777344\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "mask_lms = {}\n",
    "\n",
    "i = 0\n",
    "while len(mask_lms) < mask_cnt:\n",
    "    dice = random.random()\n",
    "    for index in cand_idx[i]:\n",
    "        mask_lms[index] = tokens[index]\n",
    "        if dice < 0.8:\n",
    "            tokens[index] = \"[MASK]\"\n",
    "        elif dice < 0.9:\n",
    "            tokens[index] = random.choice(vocab_list)\n",
    "        # else: 위 두 조건에 해당하지 않을 확률이 10%므로 아무것도 하지 않으면 10% 확률로 원본 그대로 유지\n",
    "    i += 1\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a8541",
   "metadata": {},
   "source": [
    "코드골프한 코드의 실행속도입니다.\n",
    "\n",
    "오.. 상당히 흡족스러운 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf69e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28290748596191406\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    mask_lms = []  # mask 된 값\n",
    "\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "              break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "              continue\n",
    "        # 마스크 카운트를 넘으면 종료\n",
    "\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "        # 토큰했는지 카운트는 토큰 단위로 세지만 마스킹은 공백(_) 단위로 마스킹\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083144a9",
   "metadata": {},
   "source": [
    "반복문으로 실행했을 때 예제의 코드는 약 0.28초가 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96727fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18325209617614746\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    mask_lms = {}\n",
    "\n",
    "    i = 0\n",
    "    while len(mask_lms) < mask_cnt:\n",
    "        dice = random.random()\n",
    "        for index in cand_idx[i]:\n",
    "            mask_lms[index] = tokens[index]\n",
    "            if dice < 0.8:\n",
    "                tokens[index] = \"[MASK]\"\n",
    "            elif dice < 0.9:\n",
    "                tokens[index] = random.choice(vocab_list)\n",
    "            # else: 위 두 조건에 해당하지 않을 확률이 10%므로 아무것도 하지 않으면 10% 확률로 원본 그대로 유지\n",
    "        i += 1\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ebbfe",
   "metadata": {},
   "source": [
    "코드골프한 코드는 약 0.18초로 반복문으로 실행해도 훨씬 빠르게 측정됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd902329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '[MASK]', '[MASK]', '[MASK]', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '[MASK]', '[MASK]', '[MASK]', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '[MASK]', '[MASK]', '[MASK]', '▁적', '셔', '▁몇', '[MASK]', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [1, 2, 3, 4, 18, 19, 20, 38, 39, 40, 71, 72, 73, 77]\n",
      "mask_label : ['▁추', '적', '추', '적', '▁손', '님', '이', '▁받아', '보', '는', '▁한', '잔', '을', '▁달']\n",
      "\n",
      "0.0008645057678222656\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "def create_pretrain_mask_orig(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 현재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n",
    "\n",
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask_orig(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)\n",
    "\n",
    "print()\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9bea59",
   "metadata": {},
   "source": [
    "다음은 위의 두 과정을 함수로 합한 코드입니다.\n",
    "\n",
    "예제의 코드는 약 0.0008초가 소요됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2165b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '[MASK]', '[MASK]', '[MASK]', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '梨', '▁최', '▁사무', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '[MASK]', '[MASK]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '[MASK]', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [18, 19, 20, 32, 33, 38, 39, 40, 48, 49, 50, 62, 63, 76, 85]\n",
      "mask_label : ['▁손', '님', '이', '▁오', '십', '▁받아', '보', '는', '▁서', '푼', '에', '▁흘', '러', '▁몇', '▁아내']\n",
      "0.0009834766387939453\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    startwordtoken = [i for i, token in enumerate(tokens) if token.startswith(u\"\\u2581\")] + [len(tokens)]\n",
    "    cand_idx = [[k for k in range(i, j) if tokens[k] != \"[SEP]\"] for i, j in zip(startwordtoken[:-1], startwordtoken[1:])]\n",
    "    \n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = {}\n",
    "\n",
    "    i = 0\n",
    "    while len(mask_lms) < mask_cnt:\n",
    "        dice = random.random()\n",
    "        for index in cand_idx[i]:\n",
    "            mask_lms[index] = tokens[index]\n",
    "            if dice < 0.8:\n",
    "                tokens[index] = \"[MASK]\"\n",
    "            elif dice < 0.9:\n",
    "                tokens[index] = random.choice(vocab_list)\n",
    "            # else: 위 두 조건에 해당하지 않을 확률이 10%므로 아무것도 하지 않으면 10% 확률로 원본 그대로 유지\n",
    "        i += 1\n",
    "        \n",
    "    mask_lms = dict(sorted(mask_lms.items()))\n",
    "    mask_idx, mask_label = list(mask_lms.keys()), list(mask_lms.values())\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n",
    "\n",
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acc7b1f",
   "metadata": {},
   "source": [
    "코드골프한 코드의 결과입니다.\n",
    "\n",
    "엥? 더 느려졌습니다.. 리스트 두 개의 영향일까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "827635ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.735884666442871\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    tokens = copy.deepcopy(tokens_org)\n",
    "    tokens, mask_idx, mask_label = create_pretrain_mask_orig(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d88a71",
   "metadata": {},
   "source": [
    "예제의 코드는 반복문으로 실행 시 약 2.73초입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea106f09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9512112140655518\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    tokens = copy.deepcopy(tokens_org)\n",
    "    tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d9a37",
   "metadata": {},
   "source": [
    "코드골프한 코드는 약 2.95초로 확실히 느려졌습니다.\n",
    "\n",
    "일단은 좀 더 지켜보도록 하겠습니다.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe025a8",
   "metadata": {},
   "source": [
    "- - -\n",
    "## NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48920a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33c106ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a0642",
   "metadata": {},
   "source": [
    "다음과 같은 단락이 있다고 칩시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0c3b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8 # 이거 안씀 ㅋㅋㅋ\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c71952a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "1\n",
      "tokens_a: 11 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어']\n",
      "tokens_b: 51 ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "5\n",
      "tokens_a: 55 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라']\n",
      "tokens_b: 16 ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "3\n",
      "tokens_a: 43 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와']\n",
      "tokens_b: 30 ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n",
      "0.0061533451080322266\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "        # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다.\n",
    "        \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        print(a_end)\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "    # 문장과 토큰의 수를 차례대로 담아 토큰의 수가 max_seq의 수를 넘으면 해당 문장까지를 한 문장으로 생성\n",
    "    # 1부터 len(문장)까지에서 랜덤의 수를 뽑아 문장a와 문장b로 나눔\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244be219",
   "metadata": {},
   "source": [
    "단락 내에서 정해둔 길이만큼을 잘라내 문장 A와 B를 추출하고 50%의 확률로 문장의 앞과 뒤를 바꿔주는 함수입니다.\n",
    "\n",
    "위는 예제의 코드로 약 0.0061초가 소요됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb53e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "tokens_a: 34 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']\n",
      "tokens_b: 28 ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "tokens_a: 27 ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 44 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "tokens_a: 56 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "tokens_b: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n",
      "0.001302480697631836\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i+1 == len(doc) or current_length >= max_seq):\n",
    "        # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다.\n",
    "        \n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        a_end = random.randrange(1, max(1, len(current_chunk))) # 1 or len(current_chunk) 안에서 바로 랜덤 정수 생성\n",
    "            \n",
    "        tokens_a = sum(current_chunk[:a_end], []) # extend 대신 sum, []를 이용해 슬라이싱으로 2차원 리스트 결합\n",
    "        tokens_b = sum(current_chunk[a_end:], [])\n",
    "\n",
    "        if round(random.random()): # 0에서 1 사이의 난수를 round로 반올림해서 바로 0 또는 1의 값을 가지도록 함\n",
    "            tokens_a, tokens_b = tokens_b, tokens_a\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "    # 문장과 토큰의 수를 차례대로 담아 토큰의 수가 max_seq의 수를 넘으면 해당 문장까지를 한 문장으로 생성\n",
    "    # 1부터 len(문장)까지에서 랜덤의 수를 뽑아 문장a와 문장b로 나눔\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ac90b",
   "metadata": {},
   "source": [
    "코드골프한 코드의 실행 속도입니다.\n",
    "\n",
    "더 빨라지긴 했지만 난수 생성기의 값에 따라서 실행 속도가 달라져서 정상적인 비교는 되지 못했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e42f13c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 50 ['날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 11 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 49 ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 0\n",
      "tokens_a: 44 ['▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']\n",
      "tokens_b: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 1\n",
      "tokens_a: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "tokens_b: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "\n",
      "0.003497600555419922\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b) # 두 토큰의 수가 max_seq을 넘어가면 while 종료\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0] # a문장이 길면 a문장의 앞을 제거\n",
    "        else:\n",
    "            tokens_b.pop() # b문장이 길면 b문장의 뒤를 제거\n",
    "            \n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        #if 1 < len(current_chunk):\n",
    "        #    a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0     #False\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1    #True\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "    # 이전 함수에서 50% 확률로 swap 추가\n",
    "    # is_next = 0 : 서로 맞지 않는 문장\n",
    "    # is_next = 1 : 정상적인 문장\n",
    "    \n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f84c3",
   "metadata": {},
   "source": [
    "위의 함수에서 `is_next`라는 문장의 앞 뒤가 변경됐다는 레이블이 추가되고, 문장의 길이가 61을 초과하지 않게 잘라줍니다.\n",
    "\n",
    "예제의 코드는 약 0.0034초가 소요됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ce1813d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 5 62 [['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'], ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'], ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전'], ['▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']]\n",
      "is_next: 0\n",
      "tokens_a: 50 ['날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러']\n",
      "tokens_b: 11 ['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어']\n",
      "\n",
      "current_chunk: 6 71 [['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내'], ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던'], ['▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어'], ['▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라'], ['▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']]\n",
      "is_next: 0\n",
      "tokens_a: 49 ['▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가']\n",
      "tokens_b: 12 ['▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔']\n",
      "\n",
      "current_chunk: 5 73 [['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던'], ['▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와'], ['▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져']]\n",
      "is_next: 1\n",
      "tokens_a: 17 ['▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던']\n",
      "tokens_b: 44 ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어']\n",
      "\n",
      "current_chunk: 2 22 [['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 13 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날']\n",
      "\n",
      "0.0023899078369140625\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "            \n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i+1 == len(doc) or current_length >= max_seq):\n",
    "        # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다.\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #a_end = random.randrange(1, max(1, len(current_chunk)))\n",
    "        a_end = 1\n",
    "            \n",
    "        tokens_a = sum(current_chunk[:a_end], [])\n",
    "        tokens_b = sum(current_chunk[a_end:], [])\n",
    "\n",
    "        is_next = round(random.random()) # is_next 변수만 추가\n",
    "        if not is_next:\n",
    "            tokens_a, tokens_b = tokens_b, tokens_a\n",
    "\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        for _ in range(max(0, (len(tokens_a) + len(tokens_b)) - max_seq)): # trim_tokens 함수를 쓰지 않고 반복문 하나 생성\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                del tokens_a[0] # a문장이 길면 a문장의 앞을 제거\n",
    "            else:\n",
    "                tokens_b.pop() # b문장이 길면 b문장의 뒤를 제거\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "    # 이전 함수에서 50% 확률로 swap 추가\n",
    "    # is_next = 0 : 서로 맞지 않는 문장\n",
    "    # is_next = 1 : 정상적인 문장\n",
    "    \n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb419e8",
   "metadata": {},
   "source": [
    "`trim_tokens` 함수를 제거하고 대신 반복문 하나로 대신했습니다.\n",
    "\n",
    "정확한 시간 비교를 위해 두 함수 전부 a_end : 1 고정하고 약 0.0023초로 더 빠른 실행속도를 확인했습니다.\n",
    "\n",
    "토큰의 총 수가 61을 초과하지 않게 정제하는 것도 잘 작동하는 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6b0193c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.442508459091187\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 16):\n",
    "    instances = []\n",
    "    current_chunk = []  # line 단위 tokens\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):  # doc 전체를 loop\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "            #print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            \n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            #print(\"is_next:\", is_next)\n",
    "            #print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "            #print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "            #######################################\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            #print(\"tokens:\", len(tokens), tokens)\n",
    "            #print(\"segment:\", len(segment), segment)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask_orig(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "            #print(\"masked tokens:\", len(tokens), tokens)\n",
    "            #print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "            #print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            #######################################\n",
    "            #print()\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        # 이전 함수에서 CLS, SEP 토큰과 segment 임베딩 생성\n",
    "        # 마스크 생성\n",
    "        # instance 정보 생성\n",
    "    \n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf324f3",
   "metadata": {},
   "source": [
    "MLM 함수와 합치며 토큰화까지 거치는 최종적인 함수를 만들었고 바로 반복문으로 실행속도를 확인해봤습니다.\n",
    "\n",
    "예제의 코드는 약 9.44초가 소요됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3367bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.622688055038452\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 16):\n",
    "    instances = []\n",
    "    current_chunk = []  # line 단위 tokens\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(doc)):  # doc 전체를 loop\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i+1 == len(doc) or current_length >= max_seq):\n",
    "            # 마지막 줄 이거나 길이가 max_seq 이상 인 경우, 학습 데이터를 만듭니다.\n",
    "            #print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "            a_end = random.randrange(1, max(1, len(current_chunk)))\n",
    "\n",
    "            tokens_a = sum(current_chunk[:a_end], [])\n",
    "            tokens_b = sum(current_chunk[a_end:], [])\n",
    "\n",
    "            is_next = round(random.random())\n",
    "            if not is_next:\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            for _ in range(max(0, (len(tokens_a) + len(tokens_b)) - max_seq)): # trim_tokens 함수를 쓰지 않고 반복문 하나 생성\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    del tokens_a[0] # a문장이 길면 a문장의 앞을 제거\n",
    "                else:\n",
    "                    tokens_b.pop() # b문장이 길면 b문장의 뒤를 제거\n",
    "                    \n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            #print(\"is_next:\", is_next)\n",
    "            #print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "            #print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "            #######################################\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            #print(\"tokens:\", len(tokens), tokens)\n",
    "            #print(\"segment:\", len(segment), segment)\n",
    "\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "            #print(\"masked tokens:\", len(tokens), tokens)\n",
    "            #print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "            #print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            #######################################\n",
    "            #print()\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "        # 이전 함수에서 CLS, SEP 토큰과 segment 임베딩 생성\n",
    "        # 마스크 생성\n",
    "        # instance 정보 생성\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ec6cc",
   "metadata": {},
   "source": [
    "코드골프의 결과입니다.\n",
    "\n",
    "약 10.62초로 더 느립니다... 좀 충격적이네요.\n",
    "\n",
    "새로운 내용 중에서는 딱히 건드릴 것도 없군요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d28b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '▁전', '[SEP]', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [8, 9, 10, 30, 42, 43, 44, 45, 46, 47], 'mask_label': ['▁날', '이었', '어', '▁번', '▁전', '짜', '리', '▁백', '통', '화']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '[MASK]', '▁문', '득', '▁떠', '올', '라', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [17, 18, 24, 25, 26, 27, 28, 51, 57, 58], 'mask_label': ['▁전', '부터', '▁생각', '에', '▁그', '토', '록', '▁난', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '▁내', '[MASK]', '[MASK]', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '[MASK]', '[MASK]', '▁달', '라', '던', '[SEP]', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 17, 18, 27, 28, 46, 47, 48, 49], 'mask_label': ['▁옆', '에', '▁그리', '도', '▁들어', '와', '▁원', '망', '하', '듯']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '[MASK]', '[MASK]', '▁얼마', '나', '▁좋', '을', '까', '[SEP]', '▁난', '▁몰', '라', '▁오늘', '은', '[MASK]', '[MASK]', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 20, 21], 'mask_label': ['▁있', '으면', '▁운', '수']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f9d9f",
   "metadata": {},
   "source": [
    "일단 결과는 같은 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21201040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances_orig(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask_orig(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances\n",
    "\n",
    "# 최종 함수, return : instances(정보들)[tokens, segment, is_next, mask_idx, mask_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e7a44",
   "metadata": {},
   "source": [
    "예제의 코드들로만 이루어진 오리지널 함수를 `create_pretrain_instances_orig`라는 이름으로 생성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5708c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i+1 == len(doc) or current_length >= max_seq):\n",
    "            \n",
    "            a_end = random.randrange(1, max(1, len(current_chunk)))\n",
    "            \n",
    "            tokens_a = sum(current_chunk[:a_end], [])\n",
    "            tokens_b = sum(current_chunk[a_end:], [])\n",
    "            \n",
    "            is_next = round(random.random())\n",
    "            if not is_next:\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "                \n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            for _ in range(max(0, (len(tokens_a) + len(tokens_b)) - max_seq)):\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    del tokens_a[0] # a문장이 길면 a문장의 앞을 제거\n",
    "                else:\n",
    "                    tokens_b.pop() # b문장이 길면 b문장의 뒤를 제거\n",
    "                    \n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            \n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances\n",
    "\n",
    "# 최종 함수, return : instances(정보들)[tokens, segment, is_next, mask_idx, mask_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49c8b6",
   "metadata": {},
   "source": [
    "그리고 코드골프한 함수는 `create_pretrain_instances`란 이름으로 함수를 생성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c7b9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4844255447387695\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    instances = create_pretrain_instances_orig(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b269df",
   "metadata": {},
   "source": [
    "오리지널 함수의 반복 실행속도는 약 4.48초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47c192a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.071884393692017\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49e5dd",
   "metadata": {},
   "source": [
    "코드골프 함수의 반복 실행속도는 약 5.07초로 더 느립니다.\n",
    "\n",
    "리스트 두 개를 포기해야겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d79a9eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '[MASK]', '[MASK]', '[MASK]', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', '[MASK]', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '師', '싫', '妄', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 17, 18, 19, 27, 30, 44, 45, 46], 'mask_label': ['적', '추', '적', '▁손', '님', '이', '▁전', '▁번', '▁백', '통', '화']}\n",
      "{'tokens': ['[CLS]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [8, 9, 10, 24, 25, 26, 27, 28, 57, 58], 'mask_label': ['▁한', '잔', '을', '▁생각', '에', '▁그', '토', '록', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '▁달', '라', '던', '[MASK]', '[MASK]', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '어진', '콕', '▁국민', '▁', '걱', '정은', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁오늘', '은', '[MASK]', '[MASK]', '[MASK]', '▁나', '가지', '▁말', '라', '던', '▁내', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 36, 37, 38, 42, 43, 44, 48, 49, 50], 'mask_label': ['▁아내', '의', '▁떠', '올', '라', '▁더', '해', '져', '▁', '왠', '지']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '▁날', '[SEP]', '[MASK]', '▁맨', '날', '▁이렇게', '▁살', '▁수', '[MASK]', '[MASK]', '▁얼마', '나', '▁좋', '을', '까', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 17, 18], 'mask_label': ['▁난', '▁있', '으면']}\n"
     ]
    }
   ],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    \n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = {}  # mask 된 값\n",
    "    \n",
    "    i = 0\n",
    "    while len(mask_lms) < mask_cnt:\n",
    "        dice = random.random()\n",
    "        for index in cand_idx[i]:\n",
    "            mask_lms[index] = tokens[index]\n",
    "            if dice < 0.8:\n",
    "                tokens[index] = \"[MASK]\"\n",
    "            elif dice < 0.9:\n",
    "                tokens[index] = random.choice(vocab_list)\n",
    "        i += 1\n",
    "    \n",
    "    mask_lms = dict(sorted(mask_lms.items()))\n",
    "    mask_idx, mask_label = list(mask_lms.keys()), list(mask_lms.values())\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n",
    "\n",
    "            \n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i+1 == len(doc) or current_length >= max_seq):\n",
    "            \n",
    "            a_end = random.randrange(1, max(1, len(current_chunk)))\n",
    "            \n",
    "            tokens_a = sum(current_chunk[:a_end], [])\n",
    "            tokens_b = sum(current_chunk[a_end:], [])\n",
    "            \n",
    "            is_next = round(random.random())\n",
    "            if not is_next:\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "                \n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            for _ in range(max(0, (len(tokens_a) + len(tokens_b)) - max_seq)):\n",
    "                if len(tokens_a) > len(tokens_b):\n",
    "                    del tokens_a[0] # a문장이 길면 a문장의 앞을 제거\n",
    "                else:\n",
    "                    tokens_b.pop() # b문장이 길면 b문장의 뒤를 제거\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            \n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "            # 예제의 함수 사용\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances\n",
    "\n",
    "# 최종 함수, return : instances(정보들)[tokens, segment, is_next, mask_idx, mask_label]\n",
    "\n",
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83372957",
   "metadata": {},
   "source": [
    "원인으로 추정되는 리스트 두 개를 생성하는 부분만 원래의 코드를 쓰고 나머지는 코드골프한 코드들로 구성한 최종 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c95277b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.503456354141235\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    instances = create_pretrain_instances_orig(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66baa5d",
   "metadata": {},
   "source": [
    "오리지널 함수가 약 4.5초가 걸릴 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61adaa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.37716007232666\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "for _ in range(2 ** 15):\n",
    "    instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "print(time.time() - starttime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a53024",
   "metadata": {},
   "source": [
    "코드골프 함수는 약 4.37초가 걸렸습니다. 역시 리스트 두 개가 문제였습니다.\n",
    "\n",
    "앞으로 학습할 때 저는 남들보다 2초 정도씩 빠르게 완료될겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b82e4",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80c027c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3957761\n"
     ]
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    print(len([i for i in in_f]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0228f58",
   "metadata": {},
   "source": [
    "총 395만 개의 문장이 담긴 데이터셋이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99e66a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '[MASK]', '[MASK]', '[MASK]', '▁(', 'P', 'e', 'an', 'ut', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁알려', '졌다', '.', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [41, 42, 43, 49, 50, 51, 52, 53, 54, 55, 56, 57], 'mask_label': ['▁농', '부', '\"', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.']}\n",
      "{'tokens': ['[CLS]', '▁카', '터', '가', '▁해결', '한', '▁것이', '었고', ',', '▁사랑', '의', '▁집', '짓', '기', '▁운동', '▁등으로', '▁퇴', '임', '▁후에', '[MASK]', '[MASK]', '[MASK]', '▁더', '▁존', '경', '받', '는', '▁미국', '▁대통령', '▁중에', '▁특', '이', '한', '▁인물', '로', '▁남', '았다', '.', '[SEP]', '▁그는', '竹', '▁프로젝', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '[MASK]', '[MASK]', '▁인정', '받아', '▁노', '벨', '[MASK]', '[MASK]', '▁받', '게', '▁되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [19, 20, 21, 29, 40, 41, 51, 52, 57, 58], 'mask_label': ['▁', '훨', '씬', '▁중에', '▁2002', '년', '▁공', '로를', '▁평화', '상을']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '[MASK]', '[MASK]', '▁있다고', '▁한다', '.', '[MASK]', '[MASK]', '[MASK]', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '[MASK]', '[MASK]', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.', '[SEP]', '▁수학', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], 'is_next': 0, 'mask_idx': [16, 17, 21, 22, 23, 50, 51, 57, 58, 59, 60], 'mask_label': ['▁차', '이가', '▁수', '학자', '들은', '▁통해', '서', '▁파', '악', '한다', '.']}\n",
      "{'tokens': ['[CLS]', '들이', '▁동', '역', '학', '계로', '[MASK]', '[MASK]', '▁수', '▁있다', '.', '▁혼', '돈', '▁이론', '은', '[MASK]', '▁예', '측', '▁불가능', '한', '▁현', '상을', '▁탐', '구', '하는', '▁데', '▁상당', '한', '▁기여', '를', '▁한다', '.', '[SEP]', '▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '[MASK]', '[MASK]', '[MASK]', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 15, 25, 28, 29, 50, 51, 52], 'mask_label': ['▁기술', '될', '▁이러한', '▁데', '▁기여', '를', '▁집합', '론', '이']}\n",
      "\n",
      "doc: 4 instances: 1\n",
      "{'tokens': ['[CLS]', '▁수학', '▁상', '수', '[SEP]', '▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '[MASK]', '[MASK]', '授', '画', '公', '잘', '[MASK]', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.', '▁수학', '▁상', '수는', '▁대', '개', '▁실', '수', '체', '나', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원', '소', '이다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [21, 22, 23, 24, 25, 26, 27, 55, 56, 57, 58], 'mask_label': ['▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁복', '소', '수', '체의']}\n",
      "{'tokens': ['[CLS]', '▁수학', '▁상', '수', '[SEP]', '▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '[MASK]', '[MASK]', '授', '画', '公', '잘', '[MASK]', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.', '▁수학', '▁상', '수는', '▁대', '개', '▁실', '수', '체', '나', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원', '소', '이다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [21, 22, 23, 24, 25, 26, 27, 55, 56, 57, 58], 'mask_label': ['▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁복', '소', '수', '체의']}\n",
      "\n",
      "doc: 10 instances: 4\n",
      "{'tokens': ['[CLS]', '▁문학', '[SEP]', '▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '[MASK]', '[MASK]', '▁사회', '를', '▁진', '실', '되', '게', '[MASK]', '[MASK]', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '[MASK]', '[MASK]', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '[SEP]'], 'segment': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [27, 28, 35, 36, 48, 49, 50, 51, 52], 'mask_label': ['▁인간', '과', '▁묘사', '하는', '▁설명', '하면', ',', '▁언', '어를']}\n",
      "{'tokens': ['[CLS]', '하여', '▁그들의', '▁통', '속', '적인', '▁흥', '미', '와', '▁', '욕', '구를', '▁채', '워', '주는', '▁문', '학을', '▁말한다', '.', '▁대중', '문', '학의', '▁하', '위', '장', '르', '에는', '[MASK]', '[MASK]', '[MASK]', '▁있다', '.', '[SEP]', '▁문', '학을', '▁창', '작', '하는', '[MASK]', '[MASK]', '▁문', '예', '가', '라고', '▁부른다', '.', '▁문', '예', '학을', '[MASK]', '[MASK]', '▁사람을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁부른다', '.', '▁문', '학을', '▁창', '작', '하는', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [27, 28, 29, 38, 39, 49, 50, 52, 53, 54, 55], 'mask_label': ['▁여러', '가지', '가', '▁예술', '가를', '▁연구', '하는', '▁문', '예', '학자', '라고']}\n",
      "\n",
      "doc: 10 instances: 3\n",
      "{'tokens': ['[CLS]', '엮', '는', '▁것은', '▁매우', '▁어', '렵', '고', '▁논란', '이', '▁생', '길', '[MASK]', '▁있는', '▁과정', '이다', '.', '▁이', '▁목록', '을', '▁구성', '하고', '[MASK]', '▁국가', '를', '▁선정', '하는', '▁기준', '에', '▁대한', '▁정보', '는', '▁\"', '포', '함', '▁기준', '\"', '▁단', '락', '을', '▁통해', '▁설명', '하였다', '.', '[MASK]', '[MASK]', '▁대한', '▁일반적인', '▁정보', '는', '▁\"', '국', '가', '\"', '[MASK]', '[MASK]', '▁설명', '하고', '▁있다', '.', '[SEP]', '▁나라', '▁목록', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 20, 21, 22, 27, 28, 44, 45, 54, 55], 'mask_label': ['▁수', '▁구성', '하고', '▁있는', '▁기준', '에', '▁나라', '에', '▁문서', '에서']}\n",
      "{'tokens': ['[CLS]', '▁기준', '만이', '▁국가', '▁지', '위의', '▁충분', '한', '▁자', '격', '이', '든', '▁아니', '든', ',', '▁국제', '법의', '▁견', '해', '▁차', '이는', '▁존재', '할', '▁수', '[MASK]', '[MASK]', '▁이', '▁물', '음', '에', '▁대한', '▁다른', '▁이론', '에', '▁대한', '▁고', '리는', '▁아래', '에서', '▁볼', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁위', '[MASK]', '[MASK]', '▁논', '거', '하여', '▁이', '▁목록', '은', '▁다음', '▁20', '6', '개', '▁국가', '를', '▁포함', '하고', '▁있다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [24, 25, 37, 38, 40, 41, 42, 45, 46], 'mask_label': ['▁있다', '.', '▁아래', '에서', '▁수', '▁있다', '.', '▁기준', '에']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in in_f:\n",
    "        line = line.strip()\n",
    "        if line == \"\" and doc: # 빈줄일 경우 새로운 단락 시작,\n",
    "            instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "            # save\n",
    "            print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "            print(instances[0])\n",
    "            print(instances[-1])\n",
    "            print()\n",
    "            doc = []\n",
    "            \n",
    "            count -= 1\n",
    "            if not count: break\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line) # 형태소 분리\n",
    "            if pieces:\n",
    "                doc.append(pieces)\n",
    "                \n",
    "    if doc:  # 마지막에 처리되지 않은 doc가 있는 경우(마지막 단락)\n",
    "        instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []\n",
    "        \n",
    "    # 단락이 잘 나뉘는지 확인용 함수\n",
    "    # doc[0] : 단락 주제, doc[1] : 첫 번째 문장, doc[-1] : 마지막 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92a81a9",
   "metadata": {},
   "source": [
    "생각보다 충격이 커서 코드골프는 이제 포기하고 수정은 계속 하되 코드의 의미만 더 직접적으로 전달되게끔 수정하는 컨셉으로 바꿔야겠습니다.\n",
    "\n",
    "여기서부터는 딱히 수정할만한 부분도 마땅히 떠오르지 않기도 하구요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "344bb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list) # 밑에서 문락 단위로 instances 파일 생성\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        line_cnt = len([i for i in in_f])\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\" and doc:\n",
    "                    save_pretrain_instances(out_f, doc)\n",
    "                    doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if pieces:\n",
    "                        doc.append(pieces)\n",
    "            if doc:  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1bc82387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d41f3a38a741f69d525f89e50fe229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a26e18c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918189\n"
     ]
    }
   ],
   "source": [
    "# 라인수\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    total = len([i for i in f])\n",
    "    \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44af5447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "842e95ba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f03217fa4ae48fc8cf61feb99e70085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '[SEP]', '▁제임스', '▁얼', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '[MASK]', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '▁지', '미', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '[MASK]', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '賦', '▁가', '꿔', '[MASK]', '▁돈', '을', '▁벌', '었다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [8, 9, 10, 11, 24, 39, 47, 48, 49, 78, 112, 115, 121, 122, 123, 124, 125, 126], 'mask_label': ['▁\"', '지', '미', '\"', '▁~', '▁~', '▁카', '터', '는', '▁들어가', '▁등을', '▁많은', '▁그의', '▁별', '명이', '▁\"', '땅', '콩']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 4, 3324, 1042, 6, 6, 6, 6, 207, 3714, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 6, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 6, 3008, 3625, 3616, 16, 3599, 18, 3686, 6, 6, 6, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1501, 6, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 6688, 21, 5007, 6, 1927, 3607, 813, 17, 3599, 6, 6, 6, 6, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0  103 3610 3686 3718    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  203    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0  203    0    0\n",
      "    0    0    0    0    0  207 3714 3602    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 2247    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  593    0    0  399    0    0    0    0    0  307  587  931  103 4313\n",
      " 4290    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁196', '2', '년', '[MASK]', '[MASK]', '▁주', '▁상', '원', '▁의원', '▁선거', '에서', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '[MASK]', '[MASK]', '[MASK]', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '[MASK]', '[MASK]', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '[MASK]', '[MASK]', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁미국', '에', '▁사는', '▁흑', '인', '▁등', '용', '법을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [26, 27, 28, 33, 34, 52, 53, 54, 55, 69, 70, 71, 84, 85, 105, 106, 115, 116, 117, 118], 'mask_label': ['▁당선', '되었다', '.', '▁조지', '아', '▁입', '증', '하게', '▁되어', '▁낙', '선', '하지만', '▁되', '기', '▁지', '사로', '▁지', '내', '면서', ',']}\n",
      "enc_token: [5, 3306, 3625, 663, 822, 3600, 1114, 3724, 958, 3603, 117, 3674, 54, 75, 4089, 238, 1421, 9, 114, 3692, 3964, 3604, 119, 1486, 807, 2056, 6, 6, 6, 4, 386, 3619, 3625, 6, 6, 37, 76, 3667, 2378, 822, 10, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 6, 6, 6, 6, 2387, 317, 3604, 386, 3673, 3625, 1755, 3630, 37, 18, 3620, 822, 3600, 6, 6, 6, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 6, 6, 25, 1755, 3630, 3646, 76, 955, 928, 157, 3821, 61, 3773, 530, 3604, 3372, 523, 3409, 673, 1755, 3630, 6, 6, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 6, 6, 6, 6, 243, 3600, 3554, 1733, 3628, 50, 3717, 2046, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 2387   43\n",
      " 3599    0    0    0    0 1755 3630    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  213 3929  173  607\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 1567\n",
      " 3668 1447    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  450 3614    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0   18  982    0    0    0    0    0\n",
      "    0    0    0   18 3754  151 3604    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁무', '산', '되었다', '.', '[SEP]', '▁카', '터', '는', '▁이집', '트', '와', '▁이스라엘', '을', '▁조정', '하여', ',', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '[MASK]', '[MASK]', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '▁제한', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [12, 13, 14, 15, 16, 46, 47, 48, 49, 50, 77, 78, 114, 115, 116, 117, 118, 119], 'mask_label': ['▁공', '화', '당의', '▁반', '대로', '▁메', '나', '헴', '▁베', '긴', '▁유대', '인', '▁협', '상에', '▁조', '인', '했다', '.']}\n",
      "enc_token: [5, 207, 3714, 663, 3613, 1778, 3610, 570, 3607, 2270, 3653, 1003, 6, 6, 6, 6, 6, 107, 3726, 43, 3599, 4, 207, 3714, 3602, 2703, 3677, 3665, 3426, 3607, 3358, 54, 3604, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 663, 3644, 6, 6, 6, 6, 6, 1011, 3644, 280, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 330, 1487, 41, 3683, 3724, 3644, 679, 6, 6, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 456, 3928, 3708, 10, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 30, 3619, 3751, 2835, 107, 3614, 1956, 6, 6, 6, 6, 6, 6, 207, 3714, 3602, 1921, 596, 1840, 316, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0   41 3683\n",
      " 1547  141  448    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0  334 3637 5887  271 4099    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 2670 3628    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  617 1824   53 3628   31 3599    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '질', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패', '했다', '.', '▁또한', '▁임', '기', '▁말', '기에', '▁터', '진', '[MASK]', '[MASK]', '▁아', '프가', '니', '스탄', '[MASK]', '▁사건', '으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁중', '▁하나', '다', '.', '▁인', '권', '▁문제', '와', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '[MASK]', '▁한', '미', '▁관계', '가', '▁불', '편', '하기도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁대한민국', '에', '[MASK]', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '▁1982', '년까지', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [37, 38, 43, 65, 66, 67, 68, 69, 76, 77, 78, 79, 96, 104, 105, 106, 107, 110], 'mask_label': ['▁소련', '의', '▁침공', '▁지', '미', '▁카', '터', '는', '▁미', '쳤', '던', '▁대통령', '▁한때', '▁했다', '.', '▁1978', '년', '▁대한']}\n",
      "enc_token: [5, 3892, 73, 3771, 1579, 3624, 1827, 1640, 3625, 663, 822, 10, 41, 3683, 1547, 194, 4044, 3681, 1169, 3803, 958, 113, 3596, 3944, 875, 174, 2087, 1579, 31, 3599, 276, 273, 3614, 150, 329, 870, 3713, 6, 6, 26, 2986, 3733, 1323, 6, 636, 9, 751, 1640, 3625, 2219, 779, 3600, 141, 3670, 3643, 3608, 247, 3052, 4805, 3607, 114, 3692, 1853, 3599, 4, 6, 6, 6, 6, 6, 410, 786, 704, 643, 1165, 1063, 6, 6, 6, 6, 35, 324, 3598, 3599, 42, 3830, 550, 3665, 37, 3612, 3686, 3722, 380, 3636, 550, 3603, 6, 34, 3686, 704, 3608, 128, 3877, 863, 6, 6, 6, 6, 410, 3600, 6, 1876, 3601, 3038, 3600, 14, 3694, 3645, 34, 3686, 2569, 451, 3574, 421, 3604, 2760, 673, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 1302 3601    0    0    0\n",
      "    0 3232    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   18 3686  207 3714 3602\n",
      "    0    0    0    0    0    0   55 4219 3781  663    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 3590    0\n",
      "    0    0    0    0    0    0  345 3599 3331 3625    0    0   92    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '한', '▁뒤', '▁민주', '주의', '▁실', '현', '을', '[MASK]', '▁제', '▁3', '세', '계의', '[MASK]', '▁감', '시', '▁활동', '▁및', '▁기', '니', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '임을', '▁빈', '곤', '층', '쉰', '▁활동', ',', '▁사랑', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁국제', '▁분', '쟁', '▁중', '재', '▁등의', '▁활동', '도', '▁했다', '.', '[SEP]', '▁1979', '년', '▁~', '▁1980', '년', '▁대한민국의', '[MASK]', '▁격', '변', '기', '▁당시의', '▁대통령', '이었던', '[MASK]', '▁이에', '▁대해', '▁애', '매', '한', '▁태', '도를', '▁보', '였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '[MASK]', '[MASK]', '[MASK]', '▁죄', '뛰', '▁운동', '의', '▁한', '▁원', '인이', '▁', '됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박', '정', '희', '▁대통령', '이', '▁김', '재', '규', '▁중앙', '정보', '부', '장에', '▁의해', '▁살해', '된', '▁것에', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [8, 13, 18, 19, 40, 44, 49, 50, 51, 52, 53, 71, 78, 93, 94, 95, 96, 97], 'mask_label': ['▁위해', '▁선거', '▁기', '니', '▁미국의', '▁지원', '▁집', '짓', '기', '▁운동', ',', '▁정치적', '▁그는', '▁고', '조', '되는', '▁반', '미']}\n",
      "enc_token: [5, 3612, 339, 1114, 238, 158, 3756, 3607, 6, 30, 49, 3692, 1654, 6, 209, 3623, 375, 228, 24, 3733, 813, 3740, 3600, 1332, 311, 3635, 4956, 3937, 3699, 3626, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 1227, 1412, 4234, 4083, 5795, 375, 3604, 1424, 3601, 6, 6, 6, 6, 6, 605, 147, 3972, 35, 3729, 507, 375, 3627, 345, 3599, 4, 2995, 3625, 203, 1640, 3625, 447, 6, 1032, 3889, 3614, 3195, 663, 1277, 6, 695, 433, 442, 3823, 3612, 227, 701, 47, 2470, 3604, 594, 1140, 410, 3428, 6, 6, 6, 2747, 4231, 887, 3601, 34, 129, 828, 3596, 1027, 3599, 131, 3662, 981, 3629, 3604, 338, 3642, 4055, 663, 3597, 200, 3729, 3958, 782, 2275, 3638, 1312, 355, 2591, 3711, 2057, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0  231    0    0    0    0  822\n",
      "    0    0    0    0   24 3733    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0  679    0\n",
      "    0    0  770    0    0    0    0  313 4333 3614  887 3604    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 2843    0    0    0    0    0    0  202    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   70 3676  267  141 3686\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83/3610918894.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_83/3610918894.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_83/3610918894.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=5)):\n",
    "        if 5 <= i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]] # 토큰 정수 인코딩\n",
    "        enc_token += [0] * (n_seq - len(enc_token))                # 패딩\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))                    # 패딩\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label # 마스크 정보 매트릭스\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f9f5e",
   "metadata": {},
   "source": [
    "어어.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5316b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    if count:\n",
    "        total = count\n",
    "    else:\n",
    "        with open(filename, \"r\") as f:\n",
    "            total = len([i for i in f])\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if count == i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82022393",
   "metadata": {},
   "source": [
    "딱히 크게 건드린 게 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4155902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513a5897e875490c96bc2d39175e8a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83/653284519.py:40: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_83/653284519.py:41: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_83/653284519.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63c5cdbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   18, 3686,  207, 3714,    4, 3324, 1042,    6,    6,    6,\n",
       "            6,  207, 3714,   37, 3418,  416,  810, 3666, 3625,  131, 3662,\n",
       "            7, 3629,    6,  241, 3602, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1647, 3682, 3682, 3625,    6, 3008, 3625, 3616,   16,\n",
       "         3599,   18, 3686,    6,    6,    6, 1755, 3630, 3646,  630, 3714,\n",
       "         3565, 3835,  429, 3740, 3628, 3626, 1369,   10, 1605, 3599, 1755,\n",
       "         3630,   41, 3644,  830, 3624, 1135,   52, 3599,   13,   81,   87,\n",
       "         1501,    6,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,  165, 1697, 4290, 3873,\n",
       "         3703, 3683, 6688,   21, 5007,    6, 1927, 3607,  813,   17, 3599,\n",
       "            6,    6,    6,    6,    6,    6,    4], dtype=int32),\n",
       " memmap([   5, 5224,    6,    6,    6,    6, 2378, 3864, 2111,   13,  316,\n",
       "         1425,  173,  305, 3620, 1395,  149, 3607,   19,  805, 3596, 4904,\n",
       "         3750, 3603, 4065,  115, 3617, 3756, 3596, 4639, 1364, 3627,  991,\n",
       "         3616, 3600,    7, 3614, 3746,    9, 2972,  173, 1345, 3604,  848,\n",
       "         3784, 3833,    8, 3637, 2263,   12, 3614, 3746,  836, 3596, 4904,\n",
       "         3750, 3603, 4065,  115, 3600, 2972,  173,  351, 3599,    4,   62,\n",
       "         6940, 6672,    8, 3637, 3676,  848, 3784, 1931, 7194, 7647, 4363,\n",
       "         2316, 3619, 3625, 3617, 3744, 4335,   12, 3625, 3616,  175, 3662,\n",
       "            7, 3629,  203,  578, 3652, 3625, 3617, 4148, 3665,  143, 3625,\n",
       "         3616,  131, 3662,  342, 3629, 3616, 3602,  176,  334,  829, 1115,\n",
       "         3665, 1381, 4148, 3451, 1633,  375,  671,    6,    6,    6,    6,\n",
       "          765,  815, 3604,  752, 3608, 3604,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,  103, 3610, 3686,\n",
       "         3718,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  203,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,  203,    0,    0,    0,    0,\n",
       "            0,    0,    0,  207, 3714, 3602,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2247,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,  399,    0,    0,    0,    0,    0,\n",
       "          307,  587,  931,  103, 4313, 4290,    0], dtype=int32),\n",
       " memmap([   0,   13,   81, 3604,   15, 3784,   68, 3238, 3602,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  848,\n",
       "         3784, 3833,    0,    0,    0,    0,    0,    0,   58, 3676,  416,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0, 1644, 3608,  547, 3423,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]\n",
    "# pre_train_inputs[0] : 토큰 벡터\n",
    "# pre_train_inputs[1] : segment 벡터\n",
    "# pre_train_labels[0] : is_next\n",
    "# pre_train_labels[1] : 마스크 매트릭스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92fa98b",
   "metadata": {},
   "source": [
    "- - -\n",
    "## BERT 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0921bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "057b9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17bd6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d220cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f55b1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42f30fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85e33d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "809f9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40107f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fee5327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c1c91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b3ce4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "58c3c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4ef955b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 128,\n",
       " 'n_head': 2,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 512,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 1,\n",
       " 'n_seq': 128,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"d_model\": 128, \n",
    "    \"n_head\": 2, \n",
    "    \"d_head\": 64, \n",
    "    \"dropout\": 0.1, \n",
    "    \"d_ff\": 512, \n",
    "    \"layernorm_epsilon\": 0.001, \n",
    "    \"n_layer\": 1, \n",
    "    \"n_seq\": 128, \n",
    "    \"n_vocab\": 0, \n",
    "    \"i_pad\": 0\n",
    "})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ff644",
   "metadata": {},
   "source": [
    "파라미터의 개수를 100만 개로 맞추기 위해 `d_model`은 128, `n_head`는 2, `d_head`는 64, `d_ff`는 512, 인코더 블록의 수는 1개로 설정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9483a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 26s 8ms/step - loss: 9.7123 - nsp_loss: 0.7024 - mlm_loss: 9.0099 - nsp_acc: 0.4000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 9.3156 - nsp_loss: 0.6778 - mlm_loss: 8.6378 - nsp_acc: 0.6000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9630753d00>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "Epochs = 2\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=Epochs, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82727f7",
   "metadata": {},
   "source": [
    "테스트용으로 학습을 돌려보니 정상적으로 작동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917c032",
   "metadata": {},
   "source": [
    "- - -\n",
    "## BERT pre-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4a37276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef9d21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05c8b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5024750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f37f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c55a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5a6198",
   "metadata": {},
   "source": [
    "7. 프로젝트 결과\n",
    "\n",
    "학습된 모델과 학습과정을 시각화해 보세요.  \n",
    "NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a14f34a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "623119c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 128), (None, 1240064     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            16768       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 1,256,832\n",
      "Trainable params: 1,256,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de2fd7",
   "metadata": {},
   "source": [
    "저는 학습이 좀 더 오래 걸려도 괜찮을 것 같았지만 지시한 대로 약 100만 개의 파라미터를 가지는 mini BERT 모델이 완성됐습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fbf2fba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227f032",
   "metadata": {},
   "source": [
    "`train_step`은 배치 사이즈에 따라 20000이 되겠고, `warmup_step`은 `train_step`의 10분의 1인 2000이 되겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bd7a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 109s 54ms/step - loss: 21.9494 - nsp_loss: 0.6760 - mlm_loss: 21.2733 - nsp_acc: 0.5455 - mlm_lm_acc: 0.0593\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.05925, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 19.7912 - nsp_loss: 0.6408 - mlm_loss: 19.1504 - nsp_acc: 0.6020 - mlm_lm_acc: 0.1106\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.05925 to 0.11062, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 19.0845 - nsp_loss: 0.6307 - mlm_loss: 18.4539 - nsp_acc: 0.6197 - mlm_lm_acc: 0.1239\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.11062 to 0.12385, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 18.7188 - nsp_loss: 0.6250 - mlm_loss: 18.0939 - nsp_acc: 0.6266 - mlm_lm_acc: 0.1289\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.12385 to 0.12886, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 18.3460 - nsp_loss: 0.6214 - mlm_loss: 17.7245 - nsp_acc: 0.6291 - mlm_lm_acc: 0.1339\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.12886 to 0.13385, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 18.0443 - nsp_loss: 0.6188 - mlm_loss: 17.4255 - nsp_acc: 0.6339 - mlm_lm_acc: 0.1376\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.13385 to 0.13765, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 17.8551 - nsp_loss: 0.6163 - mlm_loss: 17.2388 - nsp_acc: 0.6375 - mlm_lm_acc: 0.1398\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.13765 to 0.13977, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 17.7328 - nsp_loss: 0.6148 - mlm_loss: 17.1180 - nsp_acc: 0.6413 - mlm_lm_acc: 0.1412\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.13977 to 0.14117, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 106s 53ms/step - loss: 17.6603 - nsp_loss: 0.6135 - mlm_loss: 17.0468 - nsp_acc: 0.6428 - mlm_lm_acc: 0.1419\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.14117 to 0.14193, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 107s 53ms/step - loss: 17.6310 - nsp_loss: 0.6126 - mlm_loss: 17.0183 - nsp_acc: 0.6456 - mlm_lm_acc: 0.1422\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.14193 to 0.14223, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9bb5eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419356a1",
   "metadata": {},
   "source": [
    "10 Epochs를 학습한 결과 전체 loss가 21.9에서 17.6으로 떨어지고, MLM학습과 NSP학습의 최종 정확도는 각 14%와 64%를 기록했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e8287a",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 마무리하며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02c6b83",
   "metadata": {},
   "source": [
    "### *컨셉*\n",
    "\n",
    "예제의 코드를 심도있게 보다보니 어디는 더 간단하게 수정할 수 있을 것 같고, 어디는 오류나 모순이 존재하고 하는 것들이 보이기 시작하면서\n",
    "\n",
    "프로젝트를 시작할 때 *'내가 코드를 이해한 만큼 코드를 수정하기 위해 코드골프를 해보자'* 는 컨셉으로 임했습니다.\n",
    "\n",
    "### *느낀점*\n",
    "\n",
    "평소에도 코드골프를 하면 프로그램의 실행이 빨라질까? 하는 호기심을 가지고 있었습니다.\n",
    "\n",
    "그것을 이번 기회에 확인해볼 수 있었는데, 결과는 살짝 예상은 했지만 별 차이 없었다입니다.\n",
    "\n",
    "오히려 코드골프를 하기 위한 노력과 고뇌의 시간을 대가로 보면 코드골프는 아무런 의미가 없었던겁니다.\n",
    "\n",
    "그러다보니 처음에는 코드의 줄 수를 줄이는 것이 컨셉이였지만,\n",
    "\n",
    "의미가 없다는 걸 깨달으니 점점 코드를 줄인다기보단 코드의 의미가 더 직관적으로 다가오긴 개뿔 이게 뭐죠?\n",
    "\n",
    "함수의 실행 속도가 빨라지는 것은 코드의 줄이 짧아지는 것과 별개의 일입니다.\n",
    "\n",
    "### *아쉬운점*\n",
    "\n",
    "keras에서 BERT를 사용한 예시 코드를 살펴본 적이 있는데, 그 때 BERT를 사용하는 전체적인 흐름은\n",
    "\n",
    "1. BERT의 토크나이저를 불러와서 토큰화를 한다.\n",
    "2. BERT에 토큰화한 데이터셋을 넣고 3개의 속성을 추출한다 (Token Emb, Segment Emb, Position Emb)\n",
    "3. BERT에 추출한 3개의 속성을 넣고 학습을 하면 나오는 임베딩을 얻고 다운스트림에 사용할 수 있도록 이후 레이어를 추가한다.\n",
    "\n",
    "였습니다.\n",
    "\n",
    "과정과 원리를 알면 그 부분까지 구현할 수 있을까 싶었는데 모델을 설계하는 부분은 메서드들이 너무 많아서 눈에 잘 안들어온다는 문제가 있었습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
