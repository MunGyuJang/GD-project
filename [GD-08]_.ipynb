{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e09b1d47",
   "metadata": {},
   "source": [
    "# 프로젝트 : HuggingFace 커스텀 프로젝트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be9a7d",
   "metadata": {},
   "source": [
    "## 1. Processor 만들기\n",
    "\n",
    "## 2. Tokenizer와 Model\n",
    "\n",
    "## 3. 모델 학습하기\n",
    "\n",
    "## 4. HuggingFace 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57da54ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.9/site-packages (4.8.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.21.4)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (3.19.1)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.12.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.3.4)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (2.26.0)\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: etils[epath] in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (0.9.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (5.8.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (1.5.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from tensorflow-datasets) (8.0.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.9/site-packages (from etils[epath]->tensorflow-datasets) (3.6.0)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.9/site-packages (from etils[epath]->tensorflow-datasets) (5.4.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from etils[epath]->tensorflow-datasets) (4.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.53.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c77f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from dataclasses import asdict\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600f585",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Processor 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853439bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 298.29 MiB (download: 298.29 MiB, generated: 100.56 MiB, total: 398.85 MiB) to /aiffel/tensorflow_datasets/glue/mnli/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa2b3e47d84e388ca5a2b55d37d8b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71e84c104204b09889af1110c972f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034298f23a6b43a685991da60236b5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/5 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /aiffel/tensorflow_datasets/glue/mnli/2.0.0.incompletePWDVPL/glue-train.tfrecord*...:   0%|         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched examples...:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /aiffel/tensorflow_datasets/glue/mnli/2.0.0.incompletePWDVPL/glue-validation_matched.tfrecord*...:  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched examples...:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /aiffel/tensorflow_datasets/glue/mnli/2.0.0.incompletePWDVPL/glue-validation_mismatched.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_matched examples...:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /aiffel/tensorflow_datasets/glue/mnli/2.0.0.incompletePWDVPL/glue-test_matched.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_mismatched examples...:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /aiffel/tensorflow_datasets/glue/mnli/2.0.0.incompletePWDVPL/glue-test_mismatched.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to /aiffel/tensorflow_datasets/glue/mnli/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "392702"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, info = tfds.load('glue/mnli', with_info=True)\n",
    "info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589dff07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Split('train'): <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'validation_matched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'validation_mismatched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'test_matched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>,\n",
       " 'test_mismatched': <PrefetchDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a235c4b7",
   "metadata": {},
   "source": [
    "`train`, `validation_matched`, `validation_mismatched`, `test_matched`, `test_mismatched`가 준비되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfe69ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    full_name='glue/mnli/2.0.0',\n",
       "    description=\"\"\"\n",
       "    GLUE, the General Language Understanding Evaluation benchmark\n",
       "    (https://gluebenchmark.com/) is a collection of resources for training,\n",
       "    evaluating, and analyzing natural language understanding systems.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    The Multi-Genre Natural Language Inference Corpus is a crowdsourced\n",
       "    collection of sentence pairs with textual entailment annotations. Given a premise sentence\n",
       "    and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis\n",
       "    (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are\n",
       "    gathered from ten different sources, including transcribed speech, fiction, and government reports.\n",
       "    We use the standard test set, for which we obtained private labels from the authors, and evaluate\n",
       "    on both the matched (in-domain) and mismatched (cross-domain) section. We also use and recommend\n",
       "    the SNLI corpus as 550k examples of auxiliary training data.\n",
       "    \"\"\",\n",
       "    homepage='http://www.nyu.edu/projects/bowman/multinli/',\n",
       "    data_path=PosixGPath('/tmp/tmpwr1s9umutfds'),\n",
       "    file_format=tfrecord,\n",
       "    download_size=298.29 MiB,\n",
       "    dataset_size=100.56 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'hypothesis': Text(shape=(), dtype=string),\n",
       "        'idx': int32,\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=3),\n",
       "        'premise': Text(shape=(), dtype=string),\n",
       "    }),\n",
       "    supervised_keys=None,\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test_matched': <SplitInfo num_examples=9796, num_shards=1>,\n",
       "        'test_mismatched': <SplitInfo num_examples=9847, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=392702, num_shards=1>,\n",
       "        'validation_matched': <SplitInfo num_examples=9815, num_shards=1>,\n",
       "        'validation_mismatched': <SplitInfo num_examples=9832, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{N18-1101,\n",
       "      author = \"Williams, Adina\n",
       "                and Nangia, Nikita\n",
       "                and Bowman, Samuel\",\n",
       "      title = \"A Broad-Coverage Challenge Corpus for\n",
       "               Sentence Understanding through Inference\",\n",
       "      booktitle = \"Proceedings of the 2018 Conference of\n",
       "                   the North American Chapter of the\n",
       "                   Association for Computational Linguistics:\n",
       "                   Human Language Technologies, Volume 1 (Long\n",
       "                   Papers)\",\n",
       "      year = \"2018\",\n",
       "      publisher = \"Association for Computational Linguistics\",\n",
       "      pages = \"1112--1122\",\n",
       "      location = \"New Orleans, Louisiana\",\n",
       "      url = \"http://aclweb.org/anthology/N18-1101\"\n",
       "    }\n",
       "    @article{bowman2015large,\n",
       "      title={A large annotated corpus for learning natural language inference},\n",
       "      author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},\n",
       "      journal={arXiv preprint arXiv:1508.05326},\n",
       "      year={2015}\n",
       "    }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1c312",
   "metadata": {},
   "source": [
    "대충 전제와 가설이 준비되어 있고 서로 수반하는지를 예측하는 데이터셋이라고 합니다\n",
    "\n",
    "이렇게 데이터셋을 확인하고 있으니 Exploration 프로젝트 01을 했을 때가 떠오르네요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ba7ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].take(1)\n",
    "# [hypothesis, idx, label, premise]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd8464",
   "metadata": {},
   "source": [
    "데이터셋의 shapes입니다. `premise`는 전제, `hypothesis`는 가설을 뜻합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6712ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: {hypothesis: (), idx: (), label: (), premise: ()}, types: {hypothesis: tf.string, idx: tf.int32, label: tf.int64, premise: tf.string}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test_mismatched'].take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98a597",
   "metadata": {},
   "source": [
    "저희가 예측할 테스트 데이터셋도 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9251d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : tf.Tensor(b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Meaningful partnerships with stakeholders is crucial.', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'The Clinton surrogates also held the high ground in the context war.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'The Clinton followers kept to the higher ground in the discussion. ', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b\"um-hum because women are in every field now i mean i can't think of a field that they're not involved in\", shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Women have jobs in all areas of the workforce, they are almost getting the same wages as most men,', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'Houston is really humid now', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Houston is freezing and dry right now.', shape=(), dtype=string)\n",
      "label : tf.Tensor(2, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'But not now.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b\"But they wouldn't be leaving right now. \", shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data['train'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    premise = example['premise']\n",
    "    label = example['label']\n",
    "    print(\"premise :\", premise)\n",
    "    print(\"hypothesis :\", hypothesis)\n",
    "    print(\"label :\", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1ee570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : tf.Tensor(b'uh-huh oh yeah all the people for right uh life or something', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'yeah lots of people for the right life ', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'Also, I will be assuming that the 6.0a cost of the Postal Service to take the mail from basic to workshared condition is constant as limited quantities of mail move back and forth between basic and workshared.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b' I will be assuming that the 6.0a cost of the Postal Service to take the mail from basic to workshared condition is constant.', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b\"taken up by the oh okay oh so you know well that's i had wondered sometimes i knew that there was a lot of a lot of effort and a lot of work went into a lot of that and i just wondered if if it lasted and if it took you know like yeah\", shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'It cost a lot of money to put forth all that effort.  ', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'yeah i know the motor oil', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'I know nothing about the motor oil.', shape=(), dtype=string)\n",
      "label : tf.Tensor(2, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'Many Lakeland hotels also quote a D, B and B (dinner, bed, and breakfast) rate, which includes the evening meal and is often quite cost-effective.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Many hotels in Lakeland have a cost-effective option of dinner, bed, and breakfast with an evening meal in.', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data['validation_matched'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    premise = example['premise']\n",
    "    label = example['label']\n",
    "    print(\"premise :\", premise)\n",
    "    print(\"hypothesis :\", hypothesis)\n",
    "    print(\"label :\", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c2fbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : tf.Tensor(b'Projects which enliven and enrich the student experience and draw some of our finest scholars and teachers to our campus--and to our city.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b\"These projects are largely ignored and don't impact anyone. \", shape=(), dtype=string)\n",
      "label : tf.Tensor(2, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'The center also contacted the West Virginia State Police and asked whether any reports of a downed aircraft had been received.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'The State Police in West Virginia were contacted regarding reports of fallen aircrafts.', shape=(), dtype=string)\n",
      "label : tf.Tensor(0, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b\"Don Quixote is listed in the Appendix, but I could not find him in the dictionary, nor could I find quixotic, Queen of Sheba, Jeeves, Venus, Einstein (for `genius'), Hitler (for `demagogue, tryant'), Attila the Hun (for `barbarian'), Robinson Crusoe (for `castaway; lonely person')'but man Friday is in' Horatio Alger, Narcissus , etc.\", shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Don Quixote was listed in the dictionary and not the appendix. ', shape=(), dtype=string)\n",
      "label : tf.Tensor(2, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'While the evidence of Iranian involvement is strong, there are also signs that al Qaeda played some role, as yet unknown.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'The evidence proving Iranian involvement is absolutely trustworthy.  ', shape=(), dtype=string)\n",
      "label : tf.Tensor(1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'There is nothing wrong with any of [these images] in themselves.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'These images, in themselves, are just plain wrong.', shape=(), dtype=string)\n",
      "label : tf.Tensor(2, shape=(), dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data['validation_mismatched'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    premise = example['premise']\n",
    "    label = example['label']\n",
    "    print(\"premise :\", premise)\n",
    "    print(\"hypothesis :\", hypothesis)\n",
    "    print(\"label :\", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e7f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : tf.Tensor(b'well is there a little electric pump you put in there', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Is there a pump that you put in there?', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b\"The Marquis de Sade's head is known to be above ground, as they say in the business.\", shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b\"The Marquis de Sade's head is safely kept underground.\", shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'The Galilee Experience at the marina provides a 35-minute audio-visual introduction to the region; pleasure cruisers run to Capernaeum and other places; and you can also hire small craft.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Most visitors watch the video before they set out to explore the area.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b\"He'd better ask Dorcas, or one of the maids, if he wants to know about coffee-cups. \", shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Dorcas and the maids probably know about coffee cups.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'Burton, on the ropes after a particularly tough Tony Snow ( Fox News Sunday ) question, retreats with the most transparent wiggle of the weekend.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Burton was given a hard question by Tony Snow, but he managed to evade it.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data['test_matched'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    premise = example['premise']\n",
    "    label = example['label']\n",
    "    print(\"premise :\", premise)\n",
    "    print(\"hypothesis :\", hypothesis)\n",
    "    print(\"label :\", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e834fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "premise : tf.Tensor(b'Anyway, I treasure that table.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'That table has been in my family for generations.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'The Pentagon had been struck by American 77 at 9:37:46.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b\"The Pentagon building was hit by an airplane after nine o'clock.\", shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'After evaluating the law school building the I.U.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'After evaluating the Harvard building.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'Our class has been challenged to support a program which will directly benefit the students at the IU School of Dentistry.', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'Our class is happy to support the program that benefits IU students.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n",
      "premise : tf.Tensor(b'From Oh What a Paradise it Seems , by John ', shape=(), dtype=string)\n",
      "hypothesis : tf.Tensor(b'John wrote about what a paradise it was.', shape=(), dtype=string)\n",
      "label : tf.Tensor(-1, shape=(), dtype=int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = data['test_mismatched'].take(5)\n",
    "\n",
    "for example in examples:\n",
    "    hypothesis = example['hypothesis']\n",
    "    premise = example['premise']\n",
    "    label = example['label']\n",
    "    print(\"premise :\", premise)\n",
    "    print(\"hypothesis :\", hypothesis)\n",
    "    print(\"label :\", label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15145a25",
   "metadata": {},
   "source": [
    "밸리데이션과 테스트 데이터셋은 `matched`와 `mismatched`가 있는데 정확히 무얼 뜻하는지는 모르겠습니다.\n",
    "\n",
    "그리고 테스트 데이터셋은 레이블이 `-1`로만 이루어져 있습니다.\n",
    "\n",
    "아무래도 아직 저희가 진짜 레이블을 확인할 수는 없고 따로 제공을 하는 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fdd91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        Gets an example from a dict with tensorflow tensors.\n",
    "\n",
    "        Args:\n",
    "            tensor_dict: Keys and values should match the corresponding Glue\n",
    "                tensorflow_dataset examples.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of :class:`InputExample` for the test set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def tfds_map(self, example): # 만약 레이블의 종류가 한 개가 아니면 example.label을 get_labels에서 레이블 지정\n",
    "        \"\"\"\n",
    "        Some tensorflow_datasets datasets are not formatted the same way the GLUE datasets are. This method converts\n",
    "        examples to the correct format.\n",
    "        \"\"\"\n",
    "        if len(self.get_labels()) > 1:\n",
    "            example.label = self.get_labels()[int(example.label)]\n",
    "        return example\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            return list(csv.reader(f, delimiter=\"\\t\", quotechar=quotechar))\n",
    "\n",
    "# DataProcessor\n",
    "#└ get_example_from_tensor_dict\n",
    "#└ get_train_examples\n",
    "#└ get_dev_examples\n",
    "#└ get_test_examples\n",
    "#└ get_labels \n",
    "#└ tfds_map : Args (example), return (example) 레이블 가공\n",
    "# self._read_tsv : Args (input_file, quotechar), return (csv.reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2dc44",
   "metadata": {},
   "source": [
    "`DataProcessor` 클래스입니다.\n",
    "\n",
    "제가 이해한 바로는 여러 데이터셋을 상대할 수 있는 최고층 프로세서로 표현할 수 있을 것 같습니다.\n",
    "\n",
    "때문에 딱히 수정할 부분도, 중요한 부분도 없는 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ead37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnliProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Mnli data set (GLUE version).\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict): # [idx, premise, hypothesis, label]\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(\n",
    "            tensor_dict[\"idx\"].numpy(),\n",
    "            tensor_dict[\"premise\"].numpy().decode(\"utf-8\"),\n",
    "            tensor_dict[\"hypothesis\"].numpy().decode(\"utf-8\"),\n",
    "            str(tensor_dict[\"label\"].numpy()), # str(label)\n",
    "        )\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        print(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training, dev and test sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[3]\n",
    "            text_b = line[4]\n",
    "            label = None if set_type == \"test\" else line[0] # set_type이 \"test\"일 경우 label 제거\n",
    "            examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "        return examples\n",
    "\n",
    "# MnliProcessor(DataProcessor)\n",
    "#└ __init__ : 부모 클래스를 읽어온다.\n",
    "#└ get_example_from_tensor_dict : Args (tensor_dict), return (InputExample(idx, hypothesis, premise, label))\n",
    "#└ self._create_examples : Args (lines, set_type), return (examples) 첫 번째 인덱스는 건너뛰고 [InputExample(\"set_type-i\", hypothesis, premise, label)]\n",
    "#└ get_train_examples : self._create_examples(train.tsv)\n",
    "#└ get_dev_examples : self._create_examples(dev.tsv)\n",
    "#└ get_test_examples : self._create_examples(test.tsv)\n",
    "#└ get_labels : return ([\"0\", \"1\", \"2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189cb94f",
   "metadata": {},
   "source": [
    "`DataProcessor`의 하위 클래스입니다.\n",
    "\n",
    "저희가 `MNLI` 데이터셋에 다룰 수 있게 수정해야 할 부분들이 보입니다.\n",
    "\n",
    "`MNLI` 데이터셋을 다루는 게 목적이기 때문에 이름도 `MnliProcessor`로 변경했습니다.\n",
    "\n",
    "예제에선 `get_example_from_tensor_dict`와 `get_labels`만 활용되기 때문에 나머지 메서드들은 어떤 상황에 필요한지 잘 모르겠습니다.\n",
    "\n",
    "    가설이 먼저인가? 전제가 먼저인가? 고민해봤는데 전제가 먼저인 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feec3e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------원본데이터------\n",
      "{'hypothesis': <tf.Tensor: shape=(), dtype=string, numpy=b'Meaningful partnerships with stakeholders is crucial.'>, 'idx': <tf.Tensor: shape=(), dtype=int32, numpy=16399>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'premise': <tf.Tensor: shape=(), dtype=string, numpy=b'In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.'>}\n",
      "------processor 가공데이터------\n",
      "InputExample(guid=16399, text_a='In recognition of these tensions, LSC has worked diligently since 1995 to convey the expectations of the State Planning Initiative and to establish meaningful partnerships with stakeholders aimed at fostering a new symbiosis between the federal provider and recipients of legal services funding.', text_b='Meaningful partnerships with stakeholders is crucial.', label='1')\n"
     ]
    }
   ],
   "source": [
    "processor = MnliProcessor()\n",
    "examples = data['train'].take(1)\n",
    "\n",
    "for example in examples:\n",
    "    print('------원본데이터------')\n",
    "    print(example)  \n",
    "    example = processor.get_example_from_tensor_dict(example)\n",
    "    print('------processor 가공데이터------')\n",
    "    print(example)\n",
    "\n",
    "# processor.get_example_from_tensor_dict(dict) : InputExample 모듈로 가공(guid(idx), text_a(premise), text_b(hypothesis), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f179f6",
   "metadata": {},
   "source": [
    "`glue`에서 가져온 데이터셋을 제가 만든 프로세서에 넣으면 `InputExample(guid, text_a, text_b, label)`로 가공됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34a1df",
   "metadata": {},
   "source": [
    "- - -\n",
    "### Tokenizer와 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f9b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c638b1d88464b4f8fb0935f61fcfcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a478680d0554093b2b8bad739941d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b774fb2968486c83fee54b20c5f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/258 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca230ce5460341649845b0df271fc023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c6993436df4718a4562d9464d21625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "\n",
    "# tokenizer : typeform/distilbert-base-uncased-mnli\n",
    "# model : typeform/distilbert-base-uncased-mnli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d718d",
   "metadata": {},
   "source": [
    "저는 토크나이저와 모델로 `typeform/distilbert-base-uncased-mnli`를 가져와줬습니다.\n",
    "\n",
    "[Hugging Face Models](https://huggingface.co/models)에 있는 여러 모델들을 가져와봤지만 정상적으로 작동되는 모델은 이것 뿐이였습니다.\n",
    "\n",
    "    제가 쓸 줄 몰라서 그런걸겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceb3cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _glue_convert_examples_to_features(examples, tokenizer, processor, max_length=None, label_list=None, output_mode=\"claasification\") :\n",
    "    if max_length is None : # max_length가 None이면 max_length를 tokenizer에서 찾음\n",
    "        max_length = tokenizer.max_len\n",
    "    if label_list is None:  # label_list가 None이면 label_list를 processor에서 에서 찾음\n",
    "        label_list = processor.get_labels()\n",
    "        print(\"Using label list %s\" % (label_list))\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)} # {'0': 0, '1': 1, '2': 2}\n",
    "    labels = [label_map[example.label] for example in examples]  # examples의 labels 인코딩(원래 str 상태)\n",
    "\n",
    "    batch_encoding = tokenizer( # {\"input_ids\" : Token Emb 배열, \"attention_mask\" : pad masking 배열, \"token_type_ids\" : Segment Emb 배열}\n",
    "        [(example.text_a, example.text_b) for example in examples], # examples의 [(premise, hypothesis), ...] 배열\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "    features = []\n",
    "    for i in range(len(examples)):\n",
    "        inputs = {k: batch_encoding[k][i] for k in batch_encoding} # 각 examples 인덱스의 {input_ids, attention_mask, token_type_ids}\n",
    "\n",
    "        feature = InputFeatures(**inputs, label=labels[i]) # InputFeatures(input_ids, attention_mask, token_type_ids, label)\n",
    "        features.append(feature)\n",
    "\n",
    "    for i, example in enumerate(examples[:5]):\n",
    "        print(\"*** Example ***\")\n",
    "        print(\"guid: %s\" % (example.guid)) # id\n",
    "        print(\"features: %s\" % features[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f6e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_glue_convert_examples_to_features(examples, tokenizer, processor, max_length=None, label_list=None, output_mode=\"classification\") :\n",
    "    \"\"\"\n",
    "    :param examples: tf.data.Dataset\n",
    "    :param tokenizer: pretrained tokenizer\n",
    "    :param max_length: example의 최대 길이(기본값 : tokenizer의 max_len)\n",
    "    :param task: GLUE task 이름\n",
    "    :param label_list: 라벨 리스트\n",
    "    :param output_mode: \"regression\" or \"classification\"\n",
    "\n",
    "    :return: task에 맞도록 feature가 구성된 tf.data.Dataset\n",
    "    \"\"\"\n",
    "    examples = [processor.tfds_map(processor.get_example_from_tensor_dict(example)) for example in examples]\n",
    "            # [InputExample(idx, premise, hypothesis, label) for examples]\n",
    "    features = _glue_convert_examples_to_features(examples, tokenizer, processor, max_length)\n",
    "            # [InputFeatures(input_ids, attention_mask, token_type_ids, label) for examples]\n",
    "    label_type = tf.int64\n",
    "\n",
    "    def gen():\n",
    "        for ex in features: # features 저장\n",
    "            d = {k: v for k, v in asdict(ex).items() if v is not None} # 데이터 클래스로 변경\n",
    "            label = d.pop(\"label\") # label 분리\n",
    "            yield (d, label) # d와 label 전달\n",
    "\n",
    "    input_names = [\"input_ids\"] + tokenizer.model_input_names # ['input_ids', 'input_ids', 'attention_mask', 'token_type_ids'} ?\n",
    "\n",
    "    return tf.data.Dataset.from_generator( # 데이터셋 생성\n",
    "        gen,\n",
    "        ({k: tf.int32 for k in input_names}, label_type),\n",
    "        ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad98b8",
   "metadata": {},
   "source": [
    "이번에도 features 가공 함수를 `mnli` 태스크에 맞게 바꿔줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e44aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 16399\n",
      "features: InputFeatures(input_ids=[101, 1999, 5038, 1997, 2122, 13136, 1010, 1048, 11020, 2038, 2499, 29454, 29206, 14626, 2144, 2786, 2000, 16636, 1996, 10908, 1997, 1996, 2110, 4041, 6349, 1998, 2000, 5323, 15902, 13797, 2007, 22859, 6461, 2012, 6469, 2075, 1037, 2047, 25353, 14905, 10735, 2483, 2090, 1996, 2976, 10802, 1998, 15991, 1997, 3423, 2578, 4804, 1012, 102, 15902, 13797, 2007, 22859, 2003, 10232, 1012, 102, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "guid: 206287\n",
      "features: InputFeatures(input_ids=[101, 1996, 7207, 7505, 21799, 2015, 2036, 2218, 1996, 2152, 2598, 1999, 1996, 6123, 2162, 1012, 102, 1996, 7207, 8771, 2921, 2000, 1996, 3020, 2598, 1999, 1996, 6594, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n",
      "*** Example ***\n",
      "guid: 352707\n",
      "features: InputFeatures(input_ids=[101, 8529, 1011, 14910, 2138, 2308, 2024, 1999, 2296, 2492, 2085, 1045, 2812, 1045, 2064, 1005, 1056, 2228, 1997, 1037, 2492, 2008, 2027, 1005, 2128, 2025, 2920, 1999, 102, 2308, 2031, 5841, 1999, 2035, 2752, 1997, 1996, 14877, 1010, 2027, 2024, 2471, 2893, 1996, 2168, 12678, 2004, 2087, 2273, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "guid: 372070\n",
      "features: InputFeatures(input_ids=[101, 5395, 2003, 2428, 14178, 2085, 102, 5395, 2003, 12809, 1998, 4318, 2157, 2085, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 160184\n",
      "features: InputFeatures(input_ids=[101, 2021, 2025, 2085, 1012, 102, 2021, 2027, 2876, 1005, 1056, 2022, 2975, 2157, 2085, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf_glue_convert_examples_to_features(data['train'], tokenizer, processor=processor, max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80832d2d",
   "metadata": {},
   "source": [
    "함수에 넣은 데이터는 불러온 토크나이저로 토큰화된 후 `guid`와 `features`로 추출됩니다.\n",
    "\n",
    "`features`는 `input_ids`, `attention_mask`, `token_type_ids`, `label`로 구성되어 있습니다.\n",
    "\n",
    "`input_ids`는 토큰 임베딩, `attention_mask`는 0으로 패딩된 부분을 무시하기 위한 임베딩이며, `label`은 레이블,  \n",
    "`token_type_ids`는 하나로 합쳐진 두 문장을 구별하기 위한 임베딩으로 지금의 태스크에서 필요한 임베딩이지 않나 싶은데 \n",
    "\n",
    "    Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
    "\n",
    "오류가 나며 무시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a13e8f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  1999,  5038,  1997,  2122, 13136,  1010,  1048, 11020,\n",
      "        2038,  2499, 29454, 29206, 14626,  2144,  2786,  2000, 16636,\n",
      "        1996, 10908,  1997,  1996,  2110,  4041,  6349,  1998,  2000,\n",
      "        5323, 15902, 13797,  2007, 22859,  6461,  2012,  6469,  2075,\n",
      "        1037,  2047, 25353, 14905, 10735,  2483,  2090,  1996,  2976,\n",
      "       10802,  1998, 15991,  1997,  3423,  2578,  4804,  1012,   102,\n",
      "       15902, 13797,  2007, 22859,  2003, 10232,  1012,   102,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "examples = train_dataset.take(1)\n",
    "\n",
    "for example in examples:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec23c8",
   "metadata": {},
   "source": [
    "함수를 거쳐 나온 데이터셋도 기존 데이터셋의 shapes를 가지고 있습니다.\n",
    "\n",
    "함수에 있던 dataclasses.asdict의 영향인 것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd8ed097",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_batch = train_dataset.shuffle(100).batch(16).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3910d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 9410\n",
      "features: InputFeatures(input_ids=[101, 3934, 2029, 4372, 3669, 8159, 1998, 4372, 13149, 1996, 3076, 3325, 1998, 4009, 2070, 1997, 2256, 10418, 5784, 1998, 5089, 2000, 2256, 3721, 1011, 1011, 1998, 2000, 2256, 2103, 1012, 102, 2122, 3934, 2024, 4321, 6439, 1998, 2123, 1005, 1056, 4254, 3087, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 4506\n",
      "features: InputFeatures(input_ids=[101, 1996, 2415, 2036, 11925, 1996, 2225, 3448, 2110, 2610, 1998, 2356, 3251, 2151, 4311, 1997, 1037, 20164, 2948, 2018, 2042, 2363, 1012, 102, 1996, 2110, 2610, 1999, 2225, 3448, 2020, 11925, 4953, 4311, 1997, 5357, 2948, 2015, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=0)\n",
      "*** Example ***\n",
      "guid: 4303\n",
      "features: InputFeatures(input_ids=[101, 2123, 21864, 2595, 12184, 2003, 3205, 1999, 1996, 22524, 1010, 2021, 1045, 2071, 2025, 2424, 2032, 1999, 1996, 9206, 1010, 4496, 2071, 1045, 2424, 21864, 2595, 20214, 1010, 3035, 1997, 2016, 3676, 1010, 15333, 23047, 1010, 11691, 1010, 15313, 1006, 2005, 1036, 11067, 1005, 1007, 1010, 8042, 102, 2123, 21864, 2595, 12184, 2001, 3205, 1999, 1996, 9206, 1998, 2025, 1996, 22524, 1012, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 2325\n",
      "features: InputFeatures(input_ids=[101, 2096, 1996, 3350, 1997, 7726, 6624, 2003, 2844, 1010, 2045, 2024, 2036, 5751, 2008, 2632, 18659, 2209, 2070, 2535, 1010, 2004, 2664, 4242, 1012, 102, 1996, 3350, 13946, 7726, 6624, 2003, 7078, 3404, 13966, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=1)\n",
      "*** Example ***\n",
      "guid: 4003\n",
      "features: InputFeatures(input_ids=[101, 2045, 2003, 2498, 3308, 2007, 2151, 1997, 1031, 2122, 4871, 1033, 1999, 3209, 1012, 102, 2122, 4871, 1010, 1999, 3209, 1010, 2024, 2074, 5810, 3308, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n"
     ]
    }
   ],
   "source": [
    "# validation 데이터셋\n",
    "validation_dataset = tf_glue_convert_examples_to_features(data['validation_mismatched'], tokenizer, max_length=64, processor=processor)\n",
    "validation_dataset_batch = validation_dataset.shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c3ae165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label list ['0', '1', '2']\n",
      "*** Example ***\n",
      "guid: 3498\n",
      "features: InputFeatures(input_ids=[101, 4312, 1010, 1045, 8813, 2008, 2795, 1012, 102, 2008, 2795, 2038, 2042, 1999, 2026, 2155, 2005, 8213, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 5295\n",
      "features: InputFeatures(input_ids=[101, 1996, 20864, 2018, 2042, 4930, 2011, 2137, 6255, 2012, 1023, 1024, 4261, 1024, 4805, 1012, 102, 1996, 20864, 2311, 2001, 2718, 2011, 2019, 13297, 2044, 3157, 1051, 1005, 5119, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 404\n",
      "features: InputFeatures(input_ids=[101, 2044, 23208, 1996, 2375, 2082, 2311, 1996, 1045, 1012, 1057, 1012, 102, 2044, 23208, 1996, 5765, 2311, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 1070\n",
      "features: InputFeatures(input_ids=[101, 2256, 2465, 2038, 2042, 8315, 2000, 2490, 1037, 2565, 2029, 2097, 3495, 5770, 1996, 2493, 2012, 1996, 1045, 2226, 2082, 1997, 26556, 1012, 102, 2256, 2465, 2003, 3407, 2000, 2490, 1996, 2565, 2008, 6666, 1045, 2226, 2493, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n",
      "*** Example ***\n",
      "guid: 4542\n",
      "features: InputFeatures(input_ids=[101, 2013, 2821, 2054, 1037, 9097, 2009, 3849, 1010, 2011, 2198, 102, 2198, 2626, 2055, 2054, 1037, 9097, 2009, 2001, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=None, label=2)\n"
     ]
    }
   ],
   "source": [
    "# test 데이터셋\n",
    "test_dataset = tf_glue_convert_examples_to_features(data['test_mismatched'], tokenizer, max_length=64, processor=processor)\n",
    "test_dataset_batch = test_dataset.shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fbbd2",
   "metadata": {},
   "source": [
    "`train`과 `validation`, `test` 데이터셋을 모두 준비해줍니다.\n",
    "\n",
    "밸리데이션과 테스트 데이터셋은 `mismatched`와 `matched` 중 `mismatched`를 사용했습니다.\n",
    "\n",
    "둘 다 사용해본 결과 `matched`는 학습이 진행되지 않아서 loss가 고정되는 현상이 있습니다.\n",
    "\n",
    "    이유는 아직 모르겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74105274",
   "metadata": {},
   "source": [
    "- - -\n",
    "### 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff3c466a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification at 0x7f5623a893a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1680f3",
   "metadata": {},
   "source": [
    "제가 가져온 모델을 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2c1cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  2307      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,779\n",
      "Trainable params: 66,955,779\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66525f",
   "metadata": {},
   "source": [
    "여기서도 [Huigging Face](https://huggingface.co/models)의 일부 모델은 `model.compile`을 할 수 없는 모델이였습니다.  \n",
    "이것도 아마 제가 쓸 줄 몰라서 그런걸겁니다.\n",
    "\n",
    "그리고 레이어의 최종 출력도 `Dropout`인 것도 의아스럽습니다.\n",
    "\n",
    "model의 output을 따로 지정해줘야 하는 게 아닌가 싶기도 한데 단순한 방법으론 적용이 안돼서 포기했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca925209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "115/115 [==============================] - 53s 373ms/step - loss: 0.2147 - acc: 0.9364 - val_loss: 0.6604 - val_acc: 0.8054\n",
      "Epoch 2/5\n",
      "115/115 [==============================] - 42s 367ms/step - loss: 0.1887 - acc: 0.9413 - val_loss: 0.6481 - val_acc: 0.7992\n",
      "Epoch 3/5\n",
      "115/115 [==============================] - 41s 362ms/step - loss: 0.2011 - acc: 0.9293 - val_loss: 0.6681 - val_acc: 0.8067\n",
      "Epoch 4/5\n",
      "115/115 [==============================] - 42s 365ms/step - loss: 0.2273 - acc: 0.9201 - val_loss: 0.6218 - val_acc: 0.8037\n",
      "Epoch 5/5\n",
      "115/115 [==============================] - 42s 366ms/step - loss: 0.2206 - acc: 0.9212 - val_loss: 0.6342 - val_acc: 0.8042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f562ee52d60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝에서 배치처리를 진행한 데이터셋(xxxx_dataset_batch)을 활용\n",
    "model.fit(train_dataset_batch, epochs=5, steps_per_epoch=115,\n",
    "                validation_data=validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d006540",
   "metadata": {},
   "source": [
    "배치 사이즈는 16인데 `steps_per_epoch`는 115, 총 5번의 학습.\n",
    "\n",
    "39만개의 데이터인 것에 비하면 굉장히 짧은 학습이였습니다.\n",
    "\n",
    "그래도 loss는 이미 아니, 진작에 0.2에 수렴되어 있었습니다.\n",
    "\n",
    "pre-train 되어있다는 게 이런 의미겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d9b2db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616/616 [==============================] - 28s 45ms/step - loss: 3.6705 - acc: 0.3058\n",
      "[3.670511484146118, 0.3057784140110016]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_dataset_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b0fec",
   "metadata": {},
   "source": [
    "하지만 test 데이터셋에 대한 정확도는 1/3입니다. 이는 정답을 모두 찍은 수준입니다.\n",
    "\n",
    "제가 여기서 의문이 드는 것은 테스트 데이터셋의 레이블은 모두 `-1`이였습니다. 그래서 정확도가 0이 나올거라 예상했는데 말이죠.\n",
    "\n",
    "예상가는 부분이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e6947b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([ 101, 4312, 1010, 1045, 8813, 2008, 2795, 1012,  102, 2008, 2795,\n",
      "       2038, 2042, 1999, 2026, 2155, 2005, 8213, 1012,  102,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  1996, 20864,  2018,  2042,  4930,  2011,  2137,  6255,\n",
      "        2012,  1023,  1024,  4261,  1024,  4805,  1012,   102,  1996,\n",
      "       20864,  2311,  2001,  2718,  2011,  2019, 13297,  2044,  3157,\n",
      "        1051,  1005,  5119,  1012,   102,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2044, 23208,  1996,  2375,  2082,  2311,  1996,  1045,\n",
      "        1012,  1057,  1012,   102,  2044, 23208,  1996,  5765,  2311,\n",
      "        1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2256,  2465,  2038,  2042,  8315,  2000,  2490,  1037,\n",
      "        2565,  2029,  2097,  3495,  5770,  1996,  2493,  2012,  1996,\n",
      "        1045,  2226,  2082,  1997, 26556,  1012,   102,  2256,  2465,\n",
      "        2003,  3407,  2000,  2490,  1996,  2565,  2008,  6666,  1045,\n",
      "        2226,  2493,  1012,   102,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([ 101, 2013, 2821, 2054, 1037, 9097, 2009, 3849, 1010, 2011, 2198,\n",
      "        102, 2198, 2626, 2055, 2054, 1037, 9097, 2009, 2001, 1012,  102,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "          0,    0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  6486,  3867,  1997,  1996,  2012,  1011,  3891,  6875,\n",
      "        2308,  2366,  2011,  2358,  1012,  3870,  2188,  2031,  2445,\n",
      "        4182,  2000,  7965, 10834,  1012,   102,  6486,  1003,  1997,\n",
      "        2012,  1011,  3891,  6875,  2308,  2435,  4182,  2000,  7965,\n",
      "       10834,  1998,  2382,  1003,  2020,  2445,  2039,  2005,  9886,\n",
      "        1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2947,  4841,  2024, 11248,  2043, 28363,  2954,  2007,\n",
      "       21524,  1010,  2043, 12513,  2954,  2007, 18178,  8661,  2015,\n",
      "        1010,  2043,  6505,  2954,  2007, 13329,  2072,  7486,  1010,\n",
      "        1998,  2043,  1996,  7802,  2231,  9590,  5636,  7486,  1999,\n",
      "        2049,  2670,  3470,  1012,   102,  3568,  1010,  1996, 12513,\n",
      "        1998,  2413,  2024, 11248,  2005,  9755,  2007,  7486,  1012,\n",
      "         102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2426,  2068,  1996, 10381,  1005,  1999,  1006,  2720,\n",
      "        1012, 21863,  2078,  1005,  1055,  5413,  1007,  1998,  1996,\n",
      "       27163,  2232,  1012,   102,  2720,  1012, 21863,  2078,  1005,\n",
      "        1055, 11379,  1997, 10381,  1005,  1999,  2003,  2012,  5976,\n",
      "        2007,  7731, 15931,  1012,   102,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2520, 29454, 20822,  1005,  1055,  3722, 20446,  2487,\n",
      "       10514, 14693, 12273, 14626, 19566,  1996,  2430,  1005,  1998,\n",
      "       14638,  1005, 12612,  4119,  5307,  7027, 10489,  1012,   102,\n",
      "       29454, 20822,  7164,  7027, 10489,  2024,  2488,  2125, 15887,\n",
      "        2037,  2219,  3471,  1012,   102,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n",
      "({'input_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([  101,  2002,  2089,  2031,  2081,  2019,  3535,  2000,  2644,\n",
      "        1996,  7632, 17364,  2545,  1999,  2392,  1997,  2032,  1010,\n",
      "        2025,  9301,  2008,  2178,  2001,  3564,  2369,  2032,  1012,\n",
      "         102,  2002,  5147, 10439,  2890, 22342,  2098,  2028,  1997,\n",
      "        1996,  7632, 17364,  2545,  1012,   102,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
      "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "      dtype=int32)>}, <tf.Tensor: shape=(), dtype=int64, numpy=2>)\n"
     ]
    }
   ],
   "source": [
    "examples = test_dataset.take(10)\n",
    "\n",
    "for example in examples:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d3958",
   "metadata": {},
   "source": [
    "그렇습니다.\n",
    "\n",
    "레이블 가공을 거쳤기 때문에 존재하지 않는 `-1`이란 레이블은 `['0', '1', '2']`의 -1 인덱스에 해당하는 `2`로 변경됐던 것입니다.\n",
    "\n",
    "그리고 두 문장이 수반하는지에 상관없이 무조건 2라는 레이블을 가지니 정확도가 1/3이였던 거구요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb1a650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 3.670511\tAccuracy = 0.305778\r\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.getenv('HOME') + '/aiffel/transformers'\n",
    "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    writer.write(\"Loss = %f\\t\" %(result[0]))\n",
    "    writer.write(\"Accuracy = %f\\n\" %(result[1]))\n",
    "\n",
    "#파일에 쓴 테스트 결과 확인\n",
    "!cat ~/aiffel/transformers/eval_results.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d2cb52",
   "metadata": {},
   "source": [
    "어찌됐든 저장까지 하는 것으로 끝입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453bfcb3",
   "metadata": {},
   "source": [
    "- - -\n",
    "### HuggingFace 활용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4e4257b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1083cbc1a3d342b1ab959a7db24fdbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dccc32d814f44260b582da0fecbdcbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mnli (download: 298.29 MiB, generated: 78.65 MiB, post-processed: Unknown size, total: 376.95 MiB) to /aiffel/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24f197a13534f0a9901107cdb5e728a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /aiffel/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aa2ee32be14249b782a3d3ea429191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_mnli_dataset = load_dataset('glue', 'mnli')\n",
    "print(huggingface_mnli_dataset)\n",
    "\n",
    "# DatasetDict\n",
    "#└ train : {features, num_rows}\n",
    "#└ validation_matched : {features, num_rows}\n",
    "#└ validation_mismatched : {features, num_rows}\n",
    "#└ test_matched : {features, num_rows}\n",
    "#└ test_mismatched : {features, num_rows}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9dcb9e",
   "metadata": {},
   "source": [
    "`datasets`에서 데이터셋을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c735bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "061268afa1d849028846bc13fd367fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained(\"typeform/distilbert-base-uncased-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cebb82",
   "metadata": {},
   "source": [
    "`AutoTokenizer`와 `AutoModelForSequenceClassification`을 이용해 토크나이저와 모델을 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06ab10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.', 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him', 'One of our number will carry out your instructions minutely.', 'How do you know? All this is their information again.', \"yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range\"], 'hypothesis': ['Product and geography are what make cream skimming work. ', 'You lose the things to the following level if the people recall.', 'A member of my team will execute your orders with immense precision.', 'This information belongs to them.', 'The tennis shoes have a range of prices.'], 'label': [1, 0, 0, 0, 1], 'idx': [0, 1, 2, 3, 4]}\n",
      "{'input_ids': [[101, 17158, 2135, 6949, 8301, 25057, 2038, 2048, 3937, 9646, 1011, 4031, 1998, 10505, 1012, 102, 4031, 1998, 10505, 2024, 2054, 2191, 6949, 8301, 25057, 2147, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2017, 2113, 2076, 1996, 2161, 1998, 1045, 3984, 2012, 2012, 2115, 2504, 7910, 2017, 4558, 2068, 2000, 1996, 2279, 2504, 2065, 2065, 2027, 5630, 2000, 9131, 1996, 1996, 6687, 2136, 1996, 13980, 5630, 2000, 2655, 2000, 9131, 1037, 3124, 2013, 6420, 1037, 2059, 1037, 3313, 1037, 3124, 3632, 2039, 2000, 5672, 2032, 1998, 1037, 2309, 1037, 3124, 3632, 2039, 2000, 5672, 2032, 102, 2017, 4558, 1996, 2477, 2000, 1996, 2206, 2504, 2065, 1996, 2111, 9131, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2028, 1997, 2256, 2193, 2097, 4287, 2041, 2115, 8128, 3371, 2135, 1012, 102, 1037, 2266, 1997, 2026, 2136, 2097, 15389, 2115, 4449, 2007, 14269, 11718, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 2017, 2113, 1029, 2035, 2023, 2003, 2037, 2592, 2153, 1012, 102, 2023, 2592, 7460, 2000, 2068, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3398, 1045, 2425, 2017, 2054, 2295, 2065, 2017, 2175, 3976, 2070, 1997, 2216, 5093, 6007, 1045, 2064, 2156, 2339, 2085, 2017, 2113, 2027, 1005, 2128, 2893, 2039, 1999, 1996, 3634, 7922, 2846, 102, 1996, 5093, 6007, 2031, 1037, 2846, 1997, 7597, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "def transform(data):\n",
    "    return tokenizer( # return {input_ids, attention_mask, # Segment Emb}\n",
    "        data['premise'],\n",
    "        data['hypothesis'],\n",
    "        truncation = True, # 특정 문장이 길 경우 짧게 짜를 것인지\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = True, # Segment Emb\n",
    "        )\n",
    "  \n",
    "examples = huggingface_mnli_dataset['train'][:5]\n",
    "examples_transformed = transform(examples)\n",
    "\n",
    "print(examples)\n",
    "print(examples_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78e47e",
   "metadata": {},
   "source": [
    "여기까지만 해도 `tf_glue_convert_examples_to_features` 함수를 만들던 과정까지 완료된겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ed8b3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a31f1ce16d4b8bb61c0065241bdc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/393 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de30361d7a69441c8081280a26142d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7743069c6db04ff7abbd91d1d7cbdade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45baa5e5b6ac4725aaf1c136f4a395aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6badd8eb7a5f4248bff91783978b316f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = huggingface_mnli_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3f9df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리를 비워줍니다.\n",
    "del model\n",
    "del tokenizer\n",
    "del train_dataset_batch\n",
    "del validation_dataset_batch\n",
    "del test_dataset_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f10b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "output_dir = os.getenv('HOME') + '/aiffel/transformers'\n",
    "metric_name = 'accuracy'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = 16, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 16, # evaluation 시에 batch size\n",
    "    num_train_epochs = 1, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b28028c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595d9da018944ad0a7b49ec094fa4f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'mnli')\n",
    "\n",
    "def compute_metrics(eval_pred):    # 내부 함수 지정\n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0096b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1935, 'learning_rate': 1.9592568448500654e-05, 'epoch': 0.02}\n",
      "{'loss': 0.1773, 'learning_rate': 1.9185136897001307e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2183, 'learning_rate': 1.8777705345501956e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2137, 'learning_rate': 1.837027379400261e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2198, 'learning_rate': 1.796284224250326e-05, 'epoch': 0.1}\n",
      "{'loss': 0.218, 'learning_rate': 1.7555410691003914e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2233, 'learning_rate': 1.7147979139504566e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2105, 'learning_rate': 1.6740547588005215e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2254, 'learning_rate': 1.6333116036505868e-05, 'epoch': 0.18}\n",
      "{'loss': 0.215, 'learning_rate': 1.592568448500652e-05, 'epoch': 0.2}\n",
      "{'loss': 0.2294, 'learning_rate': 1.5518252933507173e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2096, 'learning_rate': 1.5110821382007822e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2192, 'learning_rate': 1.4703389830508477e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2127, 'learning_rate': 1.4295958279009128e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2233, 'learning_rate': 1.3888526727509778e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2121, 'learning_rate': 1.3481095176010431e-05, 'epoch': 0.33}\n",
      "{'loss': 0.2223, 'learning_rate': 1.3073663624511084e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2136, 'learning_rate': 1.2666232073011735e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2095, 'learning_rate': 1.2258800521512385e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2269, 'learning_rate': 1.185136897001304e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2227, 'learning_rate': 1.144393741851369e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2112, 'learning_rate': 1.1036505867014341e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2193, 'learning_rate': 1.0629074315514994e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2014, 'learning_rate': 1.0221642764015647e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2107, 'learning_rate': 9.814211212516298e-06, 'epoch': 0.51}\n",
      "{'loss': 0.2202, 'learning_rate': 9.40677966101695e-06, 'epoch': 0.53}\n",
      "{'loss': 0.2076, 'learning_rate': 8.999348109517601e-06, 'epoch': 0.55}\n",
      "{'loss': 0.2228, 'learning_rate': 8.591916558018254e-06, 'epoch': 0.57}\n",
      "{'loss': 0.2052, 'learning_rate': 8.184485006518904e-06, 'epoch': 0.59}\n",
      "{'loss': 0.2051, 'learning_rate': 7.777053455019557e-06, 'epoch': 0.61}\n",
      "{'loss': 0.2131, 'learning_rate': 7.369621903520209e-06, 'epoch': 0.63}\n",
      "{'loss': 0.2085, 'learning_rate': 6.962190352020861e-06, 'epoch': 0.65}\n",
      "{'loss': 0.2097, 'learning_rate': 6.554758800521513e-06, 'epoch': 0.67}\n",
      "{'loss': 0.2038, 'learning_rate': 6.147327249022165e-06, 'epoch': 0.69}\n",
      "{'loss': 0.1981, 'learning_rate': 5.739895697522817e-06, 'epoch': 0.71}\n",
      "{'loss': 0.2007, 'learning_rate': 5.332464146023468e-06, 'epoch': 0.73}\n",
      "{'loss': 0.1919, 'learning_rate': 4.92503259452412e-06, 'epoch': 0.75}\n",
      "{'loss': 0.2054, 'learning_rate': 4.5176010430247726e-06, 'epoch': 0.77}\n",
      "{'loss': 0.196, 'learning_rate': 4.110169491525424e-06, 'epoch': 0.79}\n",
      "{'loss': 0.1913, 'learning_rate': 3.702737940026076e-06, 'epoch': 0.81}\n",
      "{'loss': 0.2045, 'learning_rate': 3.2953063885267278e-06, 'epoch': 0.84}\n",
      "{'loss': 0.198, 'learning_rate': 2.8878748370273795e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1985, 'learning_rate': 2.4804432855280312e-06, 'epoch': 0.88}\n",
      "{'loss': 0.1982, 'learning_rate': 2.0730117340286834e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1897, 'learning_rate': 1.6655801825293353e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2018, 'learning_rate': 1.258148631029987e-06, 'epoch': 0.94}\n",
      "{'loss': 0.1994, 'learning_rate': 8.507170795306389e-07, 'epoch': 0.96}\n",
      "{'loss': 0.2013, 'learning_rate': 4.432855280312908e-07, 'epoch': 0.98}\n",
      "{'loss': 0.2051, 'learning_rate': 3.585397653194264e-08, 'epoch': 1.0}\n",
      "{'eval_loss': 0.7782726287841797, 'eval_accuracy': 0.817839707078926, 'eval_runtime': 191.3113, 'eval_samples_per_second': 51.393, 'eval_steps_per_second': 3.215, 'epoch': 1.0}\n",
      "{'train_runtime': 20505.4782, 'train_samples_per_second': 19.151, 'train_steps_per_second': 1.197, 'train_loss': 0.20887737825143757, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24544, training_loss=0.20887737825143757, metrics={'train_runtime': 20505.4782, 'train_samples_per_second': 19.151, 'train_steps_per_second': 1.197, 'train_loss': 0.20887737825143757, 'epoch': 1.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,                    # 학습시킬 model\n",
    "    args=training_arguments,                    # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=encoded_dataset['train'],     # training dataset\n",
    "    eval_dataset=encoded_dataset['validation_mismatched'], # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133859e",
   "metadata": {},
   "source": [
    "이번엔 39만개의 데이터를 모두 사용해서 학습하니 1번의 학습만으로도 상당히 많은 시간이 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53b27126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n",
      "/pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_90/2818771229.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_mismatched'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2113\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2114\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2290\u001b[0m                 \u001b[0mlosses_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlosses_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2292\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_across_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2293\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m                 \u001b[0mpreds_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds_host\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_pad_across_processes\u001b[0;34m(self, tensor, pad_index)\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m         \u001b[0;31m# Gather all sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m         \u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_gather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.evaluate(encoded_dataset['test_mismatched'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028280e3",
   "metadata": {},
   "source": [
    "테스트 데이터셋 평가는 오류가 나지만 어차피 테스트 데이터셋의 레이블은 `-1` 뿐이라 의미는 없으니 그냥 여기서 마무리하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b17df15",
   "metadata": {},
   "source": [
    "- - -\n",
    "## 마무리하며"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b91cc",
   "metadata": {},
   "source": [
    "https://huggingface.co/models 에 가면 모델들이 LMS의 예제랑은 다른 예시를 가지고 있고,\n",
    "\n",
    "원리를 찾아서 하기엔 LMS는 설명보다는 사용법만을 알려주고 있어서 상당히 힘들었습니다.\n",
    "\n",
    "지금도 제가 제대로 한 건지 모르겠네요.\n",
    "\n",
    "실패했던 pre-train 모델들을 사용하는 방법을 내가 못찾았다거나 데이터셋의 레이블을 불러올 수 있는 방법이 따로 있다던가 하는 찝찝함이 남아있습니다.\n",
    "\n",
    "Processor도 제가 이해한 만큼 정리하긴 했지만 InputExample이나 InputFeatures나 asdict같은 함수는 중요해보여도 결국 뭔지 모른 채 넘어갔습니다.\n",
    "\n",
    "https://huggingface.co/docs/transformers/index 에서 볼 수 있는 글은 전반적인 내용도 파악하지 못했습니다.\n",
    "\n",
    "프로젝트 지시 사항에선 BERT 쓰지 말라고 했었는데요. 모델들은 전부 BERT였습니다.\n",
    "\n",
    "프로젝트에서 말하는 BERT가 LMS의 예제에서 사용한 BERT를 쓰지 말라는 것인지 BERT가 활용된 모델을 쓰지 말라는 것인지 헷갈리게 하는 프로젝트였네요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
